{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 꼼꼼한 딜러링 논문 리뷰와 코드시스 이번 시간에 리뷰할 논문은 현대 딜러링 기반에 자유러 처리 기술에 핵심 아키텍처가 되고 있는 트랜스 포머입니다. 트랜스 포머 논문에 원래 제목은 attention is all you need. 논문은 제목에서 알 수 있듯이 트랜스 포머이라는 아키텍처에는 이 attention이라고 하는 것이 가장 메인 아이디어로서 사용이 된다는 걸 알 수 있습니다. 실제로 트랜스 포머는 attention이라는 메컨의 집을 전적으로 활용하는 아키텍처입니다. 트랜스 포머가 나오게 된 계기를 이해하기 위해서 딜러링 기반에 기계번역 발전과정에 대해 확인해 보겠습니다. 2021년 기준으로 최신 자연어 처리 쪽 고성능 모델들은 이런 트랜스 포머 아키텍처를 기반으로 하고 있습니다. 최근까지 화제가 되었던 gpt와 벌트는 모두 이런 트랜스 포머의 아키텍처를 적절히 활용하여 좋은 성능을 내고 있습니다. 대표적으로 gpt는 트랜스 포머의 dcode와 키텍처를 활용했고 벌트는 트랜스 포머의 인코도 아키텍처를 활용했다는 점이 특징입니다. 자연어 처리 테스크 중에서 가장 대표적이면서 중요한 테스크 중 하나는 기계번역입니다. 실제로 기계번역 기술의 발전 과정을 확인해 보시면 1986년도 지음에 어렴에니 제한되었고 그로부터 약 10년 정도가 지난 기회 lstm이 등장하였습니다. 이런 lstm을 활용하면 다양한 시코인스 정보를 모델링할 수 있는데요. 대표적으로 주가 예측, 주게암수 예측 등이 가능합니다. 이러한 lstm을 활용해서 2014년도에는 딜론인 기관 기술로 시코인스 시코인스가 등장하였습니다. 시코인스 시코인스는 현대의 딜론인 기술들이 다시 빠르게 나오기 시작한 시점인 2014년도에 이러한 lstm을 활용해서 고정된 크기의 컨택스트 맥터를 사용하는 방식으로 번역을 수행하는 방법을 제안하였습니다. 다만 이러한 시코인스 시코인스 모델이 나왔을 때의 시점만 하더라도 고정된 크기의 컨택스트 맥터를 쓰고 있기 때문에 소스 문장을 전부 고정된 크기의 한 맥터에다가 앞축을 할 필요가 있다는 점에서 성능적인 한계가 존재했습니다. 이후에 얻텐션 메커니즘이 제한된 논문이 나오면서 이러한 시코인스 시코인스 모델에 얻텐션 기법을 적용하여 성능을 더 끌어올릴 수가 있었고요. 이제 그 이후에 트레스 폼원 논문에서는 그냥 어린엔 자체를 사용할 필요가 없다는 아이디어로 오직 얻텐션 기법에 의존하는 아케택처를 설계했더니 성능이 훨씬 좋아지는 것을 보여주었습니다. 즉, 이 트레스 폼워를 기정으로 해서 더 이상 다양한 잘누철이 테스크에 대해서 어린엔 기반의 아케택처를 사용하지 않고 얻텐션 메커니즘을 더욱더 많이 사용하게 되었습니다. 그래서 얻텐션 메커니즘이 등장한 이유로부터는 입력 시코인스 전체에서 정보를 추출하는 방향으로 연구 방향이 발전되어왔다고 할 수 있습니다. 물론 이후에 나온 논문들 중에서도 어린엔을 활용하는 아케택처도 많이 존재하지만 전반적인 추세 자체는 얻텐션 기법을 더욱더 활용하는 이런 트레스 폼웨의 아케택처를 따르는 방식으로 다양한 고성능 모델이 제한되고 있습니다. 그렇다면 기존의 제한되었던 시코인스 시코인스 모델에는 어떤 한계점이 존재할까요? 기존 시코인스 시코인스 모델의 한계점이라고 한다면 이 컨택스트 맥터 V에 소스문장의 정보를 압축한다는 점입니다. 이때, 병목 현상이 발생할 수 있기 때문에 성능하락에 원인이 될 수 있는데요. 현재 예실을 확인해 보시면 대표적인 시코인스 시코인스 모델을 활용한 지게 번역 예시라고 할 수 있습니다. 왼쪽에 있는 독일어 문장, 즉 각각의 단어들로 구성된 하나의 시코인스가 들어왔을 때 이렇게 중간에서 하나의 고정된 크기에 컨택스트 맥터로 바꾼 뒤에 다시 이러한 컨택스트 맥터로부터 출령 문장을 만들어 낸 것을 확인할 수 있습니다. 즉, 한쪽에 시코인스에서부터 다른 한쪽에 시코인스를 만든다는 의미에서 시코인스 시코인스 모델이라고 부를 수 있습니다. 결과적으로 이렇게 영어 출령 문장에 나오는 걸 확인할 수 있고요. 다만 이때 이러한 시코인스 시코인스 아키텍처를 확인해 보시면 매번 단어가 입력될 때마다 히든스텔트 값을 갱신하는 걸 확인할 수 있습니다. 이런 식으로 단어가 입력될 때마다 이전까지 입력되었던 단어들에 대한 정보를 포함하고 있는 히든스텔트 값을 받아서 매번 이런 식으로 히든스텔트 값을 새롭게 갱신합니다. 즉, 이런 식으로 각각의 단어가 차뢰대로 순서에 맞게 입력될 때마다 히든스텔트 값이 갱신 돼요. 이러한 히든스텔트 값은 이전까지 입력되었던 단어들에 대한 정보를 갖고 있기 때문에 이렇게 마지막 단어가 들어왔을 때 그 때의 히든스텔트 값은 소스 문장 전체를 대표하는 하나의 컨택스트 백토로서 사용할 수가 있다는 것입니다. 그렇기 때문에 이렇게 마지막 단어가 들어왔을 때의 히든스텔트 값을 하나의 컨택스트 백토로서 이 컨택스트 백토로 안에는 앞에 등장했던 소스 문장에 대한 문맥적인 정보를 담고 있다고 가정하는 것입니다. 그렇기 때문에 이러한 컨택스트 백토로부터 출발해서 이렇게 출력을 수행하는 디코더 파트에서는 매번 출력 단어가 들어올 때마다 이러한 컨택스트 백토로부터 출발해서 마찬가지로 히든스텔트를 만들어서 매번 출력을 내보냅니다. 이렇게 그 다음 단계에서는 이전에 출력했던 단어가 다시 입력으로 들어와서 반복적으로 이전까지 출력했던 단어에 대한 정보를 가지고 있는 히든스텔트와 같이 입력을 받아 새롭게 히든스텔트를 갱신하는 걸 확인할 수 있습니다. 이런 식으로 디코더 파트에서는 매번 히든스텔트 값을 갱신하면서 이렇게 히든스텔트 값으로부터 출력 값이 엔도우 시컨스가 나올 때까지 반복합니다. 그래서 엔도우 시컨스가 나왔을 때 출력 문장 생성을 마치게 되고요. 이렇게 출력된 정보인 Good Evening 나오는 걸 확인할 수 있습니다. 이게 가장 기본적인 형태에 시컨스트 시컨스 모델에 동작 올리입니다. 다만 확인해 보시면 이렇게 소스 문장을 대표하는 하나의 컨택스트 백토를 만들어야 한다는 점에서 이렇게 고정된 크기에 컨택스트 백터에 정보를 압축하려고 하면 이러한 입력 문장은 어떨 때는 짧기도 하고 어떨 때는 길기도 하기 때문에 그러한 다양한 경우에 수에 대해서 항상 소스 문장의 정보를 고정된 크기로 가지고 있는 것은 전체 성능에서 병목 현상에 원인이 될 수 있습니다. 그래서 이러한 문제를 조금이나마 완화하기 위한 아이디어로 이 고정된 크기에 컨택스트 백터를 매번 이 디코드의 어레능셀에서 참고하도록 만들어서 조금 더 성능을 개선할 수 있습니다. 이렇게 하게 되면 이 컨택스트 백터에 대한 정보가 이 디코더 파트에 어레능셀을 거침에 따라서 정보가 손실되는 정도로 더 줄일 수 있기 때문에 출력되는 문장이 길어진다고 하더라도 각각의 출력되는 단호에 이러한 컨택스트 백터에 대한 정보를 다시 한번 넣어줄 수 있어서 성능이 기존보다 조금 더 향상될 수 있습니다. 다만 이런 식으로 접근한다고 하더라도 여전히 이 소스 문장을 하나의 백터에 압축해야 된다는 점은 동일하기 때문에 병목 현상은 여전히 발생합니다. 즉 현재의 문제 상황이라고 한다면 하나의 문백 백터 제 컨택스트 백터가 소스 문장의 모든 정보를 가지고 있어야 하기 때문에 성능이 저화될 수 있다는 것입니다. 그렇다면 디코더 파트에서는 하나의 문백 백터에 대한 정보만 가지고 있는 게 아니라 출력 단어를 만들 때마다 매번 소스 문장에서의 출력값들 전부를 입력으로 받으면 어떨까요? 라는 아이디어가 나올 수 있는 거죠. 최신 GPU는 많은 메모리가 그리고 빠른 병렬 처리를 지원하기 때문에 소스 문장의 시컨스 길이가 길다고 하더라도 그러한 소스 문장을 구성하는 각각에 단어에 대한 출력값들 전부를 특정현료를다가 기록해 놓았다가 소스 문장에 대한 전반적인 내용들을 매번 출력할 때마다 반영할 수 있기 때문에 성능이 좋아질 것을 기대할 수 있습니다. 다시 말해 하나의 고정된 크기에 컨택스트 백터에 담치 말고 그냥 소스 문장에서 나왔던 출력값들 전부를 매번 입력으로 받아서 1년의 처리 과정을 거쳐서 출력 단어를 만들도록 하면 성능이 더 좋아질 수 있다는 겁니다. 지금 보이는 아키텍척을 바로 시컨스 시컨스에 얕텐션 메커니즘을 적용한 아키텍척인데요. 이렇게 얕텐션 메커니즘을 적용해서 인코더 파티에 모든 출력을 잠구하도록 만들 수가 있습니다. 실제로 파이톨치와 같은 후에 먹에서는 단순히 어렸네나 LSTM 같은 걸 사용하도록 만들면 이렇게 매번 전체 시컨스 기회에 만든 아웃폭 값들이 따로 출력값들이 나오게 되는데요. 이제 그걸 그대로 이용해서 실제로 얕텐션 메커니즘을 간단하게 구현할 수도 있습니다. 전반적인 내용을 확인해 보시면 이렇게 매번 단어가 출력되어서 히든스테이트가 나올 때마다 그냥 이 값들을 전부 다 출력값으로써 그냥 별도의 배월에다가 다 기록해놓습니다. 그래서 이런 식으로 각각의 단어를 거치면서 갱심되는 히든스테이트 값들을 매번 다 가지고 있는 거예요. 이렇게 해줌으로써 이렇게 매 단어가 뒤로 왔을 때의 히든스테이트 값을 전부 가지고 있을 수 있기 때문에 이러한 값들을 어떻게든 참고해서 이렇게 출력 단어가 매번 생성될 때마다 이러한 소스 문장 전체를 반영하겠다 라는 아이디어라고 보시면 되겠습니다. 실제로는 이렇게 디코더 파트에서 매번 히든스테이트를 갱신하게 되는데 이때 현재 단계에서 히든스테이트 값을 만든다고 하면 바로 이전에 히든스테이트 값을 이용해서 이 출력 단어 히든스테이트 값과 이렇게 소스 문장단이 히든스테이트 값을 서로 묶어서 별도의 행료곱을 수행해서 각각 에너지 값을 만들어 냅니다. 이제 이때의 그 에너지 값은 내가 현재 어떤 단어를 출력하기 위해서 소스 문장에서 어떤 단어에 초점을 둘 필요가 있는지를 수치화해서 표현한 값입니다. 이제 그래서 그러한 에너지 값에 소스템 액스를 취해서 확륙 값을 구한 뒤에 그렇게 소스 문장에 값가게 히든스테이트 값에 대해서 어떤 맥터에 더 많은 가중치를 두어서 참고하면 좋을지를 반영해서 그렇게 가중치 값을 다 히든스테이트에 고판 것을 값가게 비율에 맞게 더해준 다음에 이제 그러한 웨이티드썸 값을 매번 출력 단어를 만들기 위해서 반영을 하겠다라고 보시면 됩니다. 그래서 단순히 이렇게 컨택스트 맥터만 참고하는 것이 아니라 여기에 더불어서 소스 문장에서 출력에 되었던 모든 히든스테이트 값들을 전부 반영해서 이러한 소스 문장에 단어들 중에서 어떤 단어에 더욱 더 주의 집중해서 출력 결과를 만들 수 있는 개를 우리 모델이 고려하도록 만들어서 선능을 더욱 높일 수가 있다는 겁니다. 지금 쉽게 말하면 매번 출력할 때마다 이 소스 문장에서 나왔던 모든 출력 값들을 전부 참고하겠다 이거입니다. 즉 이렇게 아축된 컨택스트 맥터 하나만 보는 것이 아니라 이런 출력 값들을 전부 고려한 하나의 웨이티드썸 맥터를 구한 다음에 개를 이렇게 같이 입력으로 넣어줘서 소스 문장에 대한 정보를 모두 고려할 수 있도록 만들기 때문에 성능이 좋아질 수 있는 것입니다. 자 그래서 실제로 이 엔템션 기법을 적용했을 때 기코더 파티에서 각각의 출력 단어를 만드는 과정을 수식적으로 표현하면 다음과 같이 정리할 수가 있는데요. 이때 아이는 현재 기코더가 철이 중인 인덕스가 되겠습니다. 즉, 기코더가 매번 한 번의 하나의 단어를 만드는데 그 각각의 철이 중인 인덕스가 아이가 되겠고요. 이때의 재위 같은 경우는 인코더가 파티에서 출력인 덕스가 되겠습니다. 즉, 에너지 값이라고 하는 것은 매번 기코더가 출력 단어를 만들 때마다 모든 재위를 고려하는 겁니다. 즉, 인코더가의 모든 출력들을 고려하겠다 라고 보시면 돼요. 이제 여기에서 S는 기코더가 이전에 출력했던 단어를 만들기 위해 사용했던 히든스텔트가 되겠고요. 여기 H는 인코더가의 각각의 인코더가 즉, 이걸 간단하게 정리하자면 기코더가 파티에서 내가 이전에 출력했던 정보는 이거인데, 이 정보와 인코더가의 모든 출력값과 비교를 해서 에너지 값을 구하겠다는 겁니다. 즉, 어떤 H 값과 가장 많은 연관성을 가지는지를 에너지 값으로 구할 수가 있는 거고 이제 이러한 에너지 값의 소프트 맥스에 추회해서 확륙 값을 구합니다. 즉, 실제로 비율적으로 이 각각의 H 값들 중에서 어떤 값과 가장 연관성이 높은지를 구하도록 만들고 이제 이러한 가중치 값을 실제로 H 값과 고파도록 만들어서 이러한 가중치가 반영된 각각의 인코더가의 출력 결과를 더해서 그것을 활용하는 것입니다. 그래서 이 그림은 실제로 이 attention mechanism을 제안한 논문에서 보여주고 있는 그림인데요. 자, 보시면 마찬가지로 에너지와 가중치의 공식은 동일합니다. 보시면, 매번 디코더 파트에서 각각의 단어를 만들기 위해서는 이런 식으로 히든스테이트 값을 이용해서 만들 수가 있는데요. 현재 히든스테이트 즉 ST를 만들기 위해서 이전에 사용했던 히든스테이트 값과 이 인코더 파트에 모든 각각의 히든스테이트 값을 같이 묶어서 에너지 값을 구한 뒤에 이제 거기에 소프트 맥스를 추회서 이렇게 비율 값을 구할 수 있는 거예요. 즉, 그러한 비율이 각각 이 A가 되겠습니다. 예를 들어서 만약에 입력 문장이 I, M, R, T 쳐라고 해볼게요. 이때 I, M, R, T 쳐의 각각의 단어를 중요해서 어떤 걸 가장 많이 참고하면 되는지를 그 비율 값을 이렇게 퍼센테이지로 구해주는 겁니다. 예를 들어서 I는 70%, M은 20%, A는 5%, T 쳐도 5%, 이런 식으로 다 더했을 때 배게 될 수 있도록 그 확률 값을 구해서 그 비율 만큼 실제로 이 A 집 값을 곱한 것을 이 컨택스트 백터처럼 사용을 할 수 있다는 겁니다. 그래서 매번 출력을 할 때마다 이렇게 소스 문장에서 출력되었던 모든 히든스테이트 값들을 전부 반영해서 이렇게 다음에 뭘 출력할지를 만들 수 있다 라는 겁니다. 이제 그래서 이렇게 매번 출력 단어를 만들 때마다 이렇게 소스 문장에서 등장했던 모든 히든스테이트 값들을 전부 반영하도록 만들어서 사용할 수가 있는 겁니다. 즉, 다시 한 번 용어를 정리해 드리자면 이 에너지 값은 소스 문장에서 나왔던 모든 출력 값들 중에서 어떤 값과 가장 영광성이 있는지를 구하기 위해서 그 수치를 구한 것이고 이제 그 값들을 소프템엑스에 넣어서 상대적인 확률 값을 구한 것이 그 가중치라고 할 수 있겠고요. 그러한 가중치 값들을 실제로 각각의 그 소스 문장의 히든스테이트와 곱해져서 전부 더해진 값을 실제로 디코더의 입력으로 같이 넣어주겠다고 보시면 되겠습니다. 이제 이런 식으로 어색션 메커니즘을 쓰게 되면 성능의 조화질 뿐만 아니라 또 추가적인 장점으로는 이러한 어색션은 시각할 수도 있다는 건데요. 이런 식으로 어색션 가중치 즉 구해진 확률 값을 이용해서 매번 출력이 나올 때마다 그 출력이 입력에서 어떤 정보를 참고했는지를 구할 수가 있습니다. 지금 보시면 이거는 영어를 불어로 번역한 예시인데요. 여기서 더 어렌지면 온도 유럽위원 이렇게 나오죠. 이 각각의 단어들이 있을 때 이제 매번 이 불어에 단어들을 출력할 때마다 이러한 입력 단어들 중에서 어떤 단어에 가장 많은 초점을 돗는지를 구할 수가 있는 겁니다. 지금 이런 식으로 밝게 표시된 부분이 확률 값인 높은 부분이라고 할 수 있습니다. 이런 식으로 매번 출력할 때마다 이 출력하는 단어가 입력 단어 중에서 어떤 단어에 더욱 많은 가중치를 두어서 어색션을 수행했는지를 구할 수가 있는 겁니다. 기본적으로 딘러닝은 매우 많은 파라미터를 가지고 있기 때문에 그러한 세부적인 파라미터를 1일이 분석하면서 어떤 원리로 동작을 했는지를 알아내기는 쉽지 않습니다. 그렇기 때문에 이렇게 어색션 메커니즘은 실제로 딘러닝이 어떤 요소에 더욱 더 많은 초점을 두어서 분류를 했는가 혹은 어떠한 데이터를 만들어 냈는가 같은 과정을 분석할 때 용의하게 사용할 수 있습니다. 그렇다면 오늘 리뷰하고 있는 트레스폼의 노무는 어떤 원리로 동작할까요? 트레스폼의 말씀 드렸듯이 현대의 딘러닝 기반 자연의 철인 애트웍크에서 핵심이 되는 노무인 중 하나입니다. 그래서 노무는 원래 제목은 attention is only needy고요. 말 그대로 attention 기법만 잘 활용해도 다양한 자연어 처리 테스트에서 좋은 성능을 얻을 수 있다는 의미입니다. 다시 말해 attention 기본만 쓰기 때문에 어린엔 CNN 등의 전혀 필요로 하지 않습니다. 진짜 말 그대로 attention 기본만 사용해서 기계 번역부터 시작해서 다양한 자연어 처리 테스트 그릴 수 있는 겁니다. 오른쪽에 보이는 그림이 원본 노무는에서 보여주고 있는 트레스폼의 아키텍처인데요. 여기 보이는 것과 같이 실제로 어린엔과 CNN 전혀 사용하지 않습니다. 물론 이런 식으로 어린엔 CNN 등을 전혀 사용하지 않는다면 문장 안에 포함되어 있는 각각의 단어들의 순서에 대한 정보를 주기가 어렵습니다. 그렇기 때문에 트레스폼은 문장 내 각각의 단어들의 대한 순서에 대한 정보를 알려주기 위해서 별도로 포지션을 인코딩을 이용해서 순서에 대한 정보를 줄 수 있습니다. 이제 이러한 아키텍처는 향후, 벌뜨나, GPT와 같은 더욱 향상된 네트워크에서도 채택이 되었고요. 또한 참고로 어린엔은 사용하지 않지만 마찬가지로 인코드와 디코드 파트로 구성되는 건 동일합니다. 또한 attention 과정을 한 번만 쓰는 게 아니라 여러 레이어를 거쳐서 반복하도록 만듭니다. 즉, 이러한 인코드가 여러분 중첩되어 즉, 앤번만큼 중첩되어 사용하도록 만든다는 건데요. 참고로 지금 보이는 그림에서 이 왼쪽 파트는 인코드가 되고 이 오른쪽 파트는 디코드가 되겠습니다. 한 번 자세한 내용을 지금부터 알아볼게요. 자, 우리가 어떤 단어 정보를 네트워크에 넣기 위해서는 일반적으로 보통 인배든 과정을 거칩니다. 그렇게 해주는 이유는 일단 맨 처음에 입력 차원 자체는 특정 언어에서 존재할 수 있는 단어의 개수와 같기 때문에 또한 그렇게 차원이 많을 뿐만 아니라 각각의 정보들은 원하드 인코딩 형태로 표현이 되기 때문에 일반적으로 네트워크에 넣을 때는 먼저 인배든 과정을 거쳐서 더욱더 적은 차원에 컨틴유어스한 값으로 표현합니다. 직업도한 실수 값으로 표현할 수가 있다는 건데요. 그래서 예를 들어 이런 식으로 IMO 티처와 같은 하나의 문장이 들어왔을 때 얘는 실제로 인푸트 인배딩 네트릭스로 만들어집니다. 이때 일반적으로 이 매트릭스는 단어의 개수만큼 행위 크기를 가지고요. 저 이런 식으로 IMO 티처라는 값이 이렇게 행 형태로 들어오게 되고 이 각각의 열 데이터는 인배딩 차원과 같은 크기에 데이터가 담긴 배열을 사용하게 됩니다. 현재 그림에서는 이런 식으로 총 4개의 단어가 존재하기 때문에 이렇게 각각의 단어들의 대해석 단어에 대한 정보를 포함하고 있는 인배든 값들을 각각 구할 수가 있다는 거죠. 이런 식으로 다 구할 수가 있는 겁니다. 이런 인배딩 뒤멘전은 모델 아키텍처를 만드는 사람이 이미 로 설정해 줄 수 있는데요. 원본 논문에서는 522 정도의 값을 사용합니다. 물론 이 값은 모델의 아키텍처를 만드는 사람마다 다르게 설정할 수가 있는 거예요. 아무튼 그래서 이런 식으로 전통적인 인배딩은 네트워케어 넣기 전에 인력값들을 인배딩 형태로 변하기 위해서 사용하는 레이어라고 볼 수 있습니다. 이때 우리가 시콘스트 시콘스와 같은 어렸넨 기반에 아키텍처를 사용한다고 하면 어렸넨을 사용하는 건 만으로도 각각의 단어가 어렸넨에 들어갈 때 순서에 맞게 들어가기 때문에 자동으로 각각의 히든스텔트 값은 순서에 대한 정보를 가지게 되는데요. 만약에 트랜스 포모와 같이 어렸넨 자체를 사용하지 않는다면 위치에 대한 정보를 주기 위해서 즉 하나의 문장에 포함되어 있는 각각에 단어 중에서 어떤 단어가 앞에 오는 것이고 어떤 단어가 뒤에 오는 것인지 그러한 정보를 알려주기 위해서는 위치에 대한 정보를 포함하고 있는 인배딩을 사용할 필요가 있습니다. 이제 이를 위해 트랜스 포모에서는 위치에 대한 정보를 인코딩하고 있는 위치인코딩 즉 포지션을 인코딩을 사용합니다. 즉 이런 식으로 인포 딘배딩 네트릭스와 같은 크기 즉 같은 디면전을 가지는 별도에 위치에 대한 정보를 가지고 있는 인코딩 정보를 넣어줘서 각각 엘리먼트 와이지로 더해줌으로써 각각의 단어가 어떤 순서를 가지는지에 대한 정보를 네토크가 알 수 있도록 만드는 것입니다. 이제 그렇게 실제로 위치에 대한 정보를까지 포함하고 있는 입력 값을 실제 어색션에 넣어줄 수 있도록 합니다. 즉 이렇게 어색션이 받는 값은 입력 문장에 대한 정보에다가 실제 위치에 대한 정보로 같이 포함되어 있는 입력 값입니다. 이제 그래서 그런 입력을 받아서 각각의 단어들을 이용해서 어색션을 수행하고요. 이제 이렇게 인코드 파트에서 수행하는 어색션은 Self-Eutension 이라고 해서 각각의 단어가 서로에게 어떤 연관성을 가지고 있는지를 구하기 위해 사용합니다. 예를 들어 이런 식으로 I-M-E-T 처라고 문장이 들어오게 되면 이 문장을 구성하는 각각에 단어오인 I-M-E-T 처�가 각각 서로에게 어색션 스코어를 구해서 각각의 단어는 다른 어떠한 단어와 높은 연관성을 가지는지에 대한 정보를 학습하도록 만들 수 있습니다. 다시 말해 이러한 어색션은 이 전반적인 입력 문장에 대한 문매게 대한 정보를 잘 학습하도록 만드는 것입니다. 또한 여기에서 추가적으로 레지 듀얼 럴린 것 같은 테크닉이 사용되는데요. 이런 레지 듀얼 럴린 같은 경우는 대표적인 이미지 분류네토크인 레지낼과 같은 레토크에서 사용되고 있는 기법으로 이렇게 어떠한 값을 레이어를 거쳐서 반복적으로 단순하게 갱신하는 것이 아니라 특정 레이어를 건너띠어서 복사가 된 값을 그대로 넣어주는 기법을 의미합니다. 이런 식으로 특정 레이어를 건너띠어서 입력할 수 있도록 만드는 것을 일반적으로 레지 듀얼 커리션이라고 부르고요. 이렇게 해줌으로써 전체 네트워크는 기존 정보를 입력 받으면서 추가적으로 자녀된 부분만 학습하도록 만들기 때문에 전반적인 학습 난이도가 낫고 그렇기 때문에 초기의 모델 수렴 속도가 높게 되고 그러니 더욱더 글러벌 업팀화를 찾을 확률이 높아지기 때문에 전반적으로 다양한 네트워크에 대해서 레지 듀얼 널링을 사용했을 때 성능이 좋아지는 걸 많이 목적할 수 있고요. 트리스 폰옷과 마찬가지로 그런 아이디어를 전적으로 재택해서 성능을 높였다고 할 수 있는 겁니다. 그래서 이렇게 오탕션을 수행해주고 나온 값과 이렇게 레지 듀얼 커리션을 이용해서 바로 더해진 값을 같이 받아서 놈을 레이저 전까지 수행한 뒤에 그 결과를 내보낼 수 있도록 만듭니다. 이것이 인코드의 동작 과정이고요. 그래서 실제로 이렇게 입력값이 들어온 유로부터 오탕션을 거치고 레지 듀얼 커리션 이후에 놈을 레이저 전 그다음에 다시 FIDFOR 레이어를 거친 다음에 마찬가지로 레지 듀얼 원리 그리고 놈을 레이저 전을 추가해서 결과적으로 하나의 인코드 레이어에서 그 결과빡을 뽑아낼 수 있습니다. 이런 식으로 오탕션과 정교와 과정을 반복하는 방식으로 여러 개의 레이어를 중척해서 사용합니다. 이때 한가지 유열점은 각각의 레이어에는 서로 다른 파라미터를 가집니다. 예를 들어 1 레이어에 1번과 레이어에 2번에 포함되어 있는 오탕션 FIDFOR 레이어에 사용되는 파라미터는 서로 다릅니다. 또한 이때 이렇게 레이어를 중척해서 사용할 수 있다는 점에서 유출할 수 있겠지만 이렇게 입력되는 각과 출력되는 각세 디멘전은 동일합니다. 이제 그래서 실조로는 다음과 같이 전체 인코드와 디코드의 아케택처를 그려볼 수 있는데요. 이렇게 입력값이 들어온 다음에 여러 개의 인코드 레이어를 반복해서 가장 마지막의 인코드에서 나오게 된 그 출력값은 이렇게 디코드에 들어가게 됩니다. 이렇게 해주는 이유는 우리가 앞서 시콘스트 시콘스 모델에 오탕션 메콘이지만 활용했을 때와 마찬가지로 디코드 파트에서는 매번 출력할 때마다 입력소스 문장 중에서 어떤 단호에게 가장 많은 초점을 줘야 되는지를 알려주기 위험입니다. 다시 말해 이렇게 디코드 파트도 마찬가지로 여러 개의 레이어로 구성이 되고 이 마지막 레이어에서 나오게 된 출력값이 바로 실제로 우리가 번역을 수행한 결과 그 출력 단어가 되는 거고요. 이때 각각의 레이어에는 이 인코드에 마지막 레이어에서 나오게 된 출력값을 입력으로 받는 것입니다. 이것이 바로 트레스 포머에 가장 기본적인 동작 방식이고요. 물론 이런 식으로 인코드 파트에서 마지막 레이어에 출력값만 받는 게 아니라 이렇게 각각의 레이어마다 출력값을 받는 기법도 존재하긴 하지만요. 아무튼 기본적인 트레스 포머의 아키텍션은 이런 식으로 인코드의 마지막 레이어에서 나오게 된 출력값을 매번 디코드의 레이어에 넣어주는 방식으로 동작합니다. 그래서 이때 디코드 또한 마찬가지로 각각 단어 정보를 받아야 해서 이어서 각 단호에 상대적인 위치에 대한 정보를 알려주기 위해 인코드 값을 추가한 뒤에 입력을 넣게 되고요. 참고로 하나의 디코드 레이어에서는 두 개의 엇텐션을 사용하는데요. 첫 번째로 보이는 엇텐션은 셀프 엇텐션으로 인코드 파트와 마찬가지로 각각의 단어들이 서로가 서로에게 어떠한 가정치를 가지는지를 구하도록 만들어서 이 출력되고 있는 문장에 대한 전반적인 표현을 학습할 수 있도록 만들고요. 이렇게 이어서 디코드 레이어에 두 번째 엇텐션에서는 인코드에 대한 정보를 엇텐션할 수 있도록 만듭니다. 다시 말해 각각의 출력 단어가 인코드의 출력 정보를 바다와 사용할 수 있도록 만듭니다. 이건 다시 말해 각각의 출력되고 있는 단어가 소스 문장에서의 어떤 단어와 연관성이 있는지를 구해주는 겁니다. 그래서 여기 보이는 엇텐션은 일반적으로 인코드 디코드 엇텐션이라고 부르고요. 그 동작 원리를 간단한 예시로 설명드리자면 예를 들어 입력 문장이 I.M.R. 티처라고 한다면 이렇게 출력 값은 차례대로 나는 선생님이다. 이런 식으로 출력을 내별게 될 건데요. 이때 출력되고 있는 단어들 예를 들어서 선생님이라고 단어를 번역한다고 하면 그 선생님이라는 단어는 I.M.R. 티처 중에서 어떤 단어와 가장 높은 연관성을 가지는지를 구할 수가 있는 겁니다. 그러한 정보를 매번 엇텐션을 통해서 계산하도록 만들어서 이렇게 인코드 파트에서 나왔던 출력 결과를 전적으로 활용하도록 네트워크를 설계할 수 있는 것입니다. 이제 그래서 디코롯도 한 마찬가지로 입력으로 들어온 입력 디메전과 이 출력 디메전이 갓도록 만들어서 각각의 디코더레이어에는 여러분 중첩해서 사용할 수 있습니다. 즉, 다시 말해 이 트레이스 포모에서는 마지막 인코드의 레이어에 출력이 모든 디코드의 레이어에 입력되는 형식으로 동작하고요. 이때 전체 레이어에 개수가 4개 1대의 예시는 다한가 같습니다. 일반적으로 이 레이어에 개수는 인코드와 디코드가 동일하도록 맞춰주는 경우가 많고요. 즉, 이렇게 인코더랑 디코드 둘 다 4개의 레이어로 구성되어 있는 걸 확인할 수 있고 이렇게 인코드 파트에서 마지막 레이어에 출력 값이 각각의 디코더레이어에 입력되는 걸 확인할 수 있습니다. 여기서 입력이 된다는 의미는 방금 그림에서 확인했던 여기 디코더 파트의 두 번째 얻텐션에서 각각의 출력 단어가 입력 단어중에서 어떤 정보와 가장 높은 영관성을 가지는 진을 계산하도록 만들어주는 바로 여기 부분에서 사용된다고 보시면 되겠습니다. 또한 말씀 드렸듯이 트레이스 포모에서도 인코드와 디코드의 구조를 따릅니다. 즉, 어렸네네 사용하지 않는다는 점이 큰 차이점이고 인코드와 디코드를 다소 사용한다는 점이 특징입니다. 여기서 재미있는 점은 원래 기본적으로 어렸네네 사용할 때는 인코더, 즉 LSTM이나 어렸네네뜩은 고정된 크기로 사용하고 이 입력 단어에 개수만큼 반복적으로 인코더 레어를 거쳐서 매번 휴대스테이트를 만들었다고 하면 트레이스 포모에서는 입력 단어 자체가 하나로 쭉 연결되어서 한 번에 입력이 되고 한 번에 그에 대한 얻텐션 값을 구한다고 할 수 있습니다. 즉, 다시 말해 어렸네네 사용했을 때와 다르게 위치에 대한 정보를 한꺼번에 넣어서 한 번에 인코더를 거칠 때마다 병렬적으로 출력 값을 구해낼 수 있기 때문에 어렸네네 사용했을 때와 비교하여 일반적으로 계산 복잡도가 더 낮게 형성됩니다. 또한 실제로 확습을 수행할 때는 이러한 입력 값의 전부를 한꺼번에 넣을 수 있기 때문에 어렸네네 사용하지 않고 확습을 진행할 수 있다는 점이 장점인데요. 다만 이제 실저로 모델해서 출력 값을 내보낼 때는 마찬가지로 이 디코더 아케택처를 여러 번 사용해서 이렇게 EOS가 나올 때까지 반복하도록 만들어서 출력 값을 구하도록 만들 수 있습니다. 보시면 이렇게 중간에 컨택스트 백토로 압축하는 과정 등이 완전히 상략이 되어 있기 때문에 네트워크 자체에서 LSTM과 같은 어렸넨 구조를 아예 사용할 필요가 없다는 점이 장점이라고 할 수 있습니다. 이제 그래서 실제로 이 멀티 헤드어템션이 뭔지 한 번 알아보도록 할게요. 이렇게 트레스 포모에서 사용되는 각각의 어연템션은 여러 개의 헤드를 가진다고 해서 멀티 헤드어템션이라고 부르는데요. 그 실제 구조는 바로 당각 같습니다. 바로 오른쪽에 보이는 그림이 멀티 헤드어템션을 보여주고 있는 그림이고요. 이때 이렇게 중간에 스케일드.프로더크 어연템션이 사용되는데요. 이러한 스케일드.프로더크 어연템션은 바로 왼쪽 그림과 같이 구성됩니다. 이때 이러한 어연템션 메커니즘을 이야기 위해서는 코리와 키, 멜류가 무엇인지 알 필요가 있는데요. 이때 코리는 무언가를 물어보는 주체입니다. 즉, 어연템션 메커니즘에 간단히 설명하면 어떠한 단어가 다른 단어들과 어떠한 연관성을 가지는지를 구하는 곳이랄 수 있는데요. 이때 물어보는 주체가 코리이고 그 물어보는 대상이 키입니다. 예를 들어, I.M.O. 티처라는 하나의 문정이 있을 때 I.M.O. 티처에 포함되어 있는 각각의 단어가 다른 단어가 얼마나 연관성을 가지는지 직정하기 위해서 셀프 어연템션을 수행할 수 있는데 이때 I.M.O. 티처 각각에 대해서 얼마나 연관성이 있는지 구한다고 치면 그때 I.가 코리가 되는 거고요. I.M.O. 티처 각각 단어들은 키가 되는 것입니다. 즉, 어떠한 단어가 다른 어떠한 단어들에 관해서 어떠한 가중치값을 가지는지 구하고자 한다면 이런 식으로 각각의 키에 대해서 어연템션 스코어를 구해 오는 방식으로 동작하는 것입니다. 이때 그렇게 스코어를 구한 뒤에는 실제로 멜룩값들과 곱해서 결과적인 어연템션 멜룩값을 구할 수 있는 겁니다. 내고 확인해 보시면 이런 식으로 물어보는 주체, 직커리가 들어오고 각각의 어연템션을 수행할 단어들 그 정보가 K로 들어가는 겁니다. 그래서 행료급을 수행한 뒤에 간단하게 스케일링을 해주고 필요하다가 마스크를 씌워준 다음에 이제 소프트 맥스를 취해서 각각의 키 중에서 어떤 단어와 가장 높은 연관성을 가지는지를 그 비율을 구할 수 있습니다. 앞에서 공부했었던 어연템션 메커니진과 갖다갈 수 있죠. 그렇게 구해진 확률값과 실제로 멜룩값을 곱해서 가중치가 적용된 결과적인 어연템션 멜료를 구할 수가 있는 겁니다. 이제 그러한 과정이 이렇게 스케일리다파라덕 어연템션에서 수행되는 것이고요. 또한 여기에서 참고로 실제로 입력값이 들어왔을 때 그런 입력값들은 HG로 구분됩니다. 지금 어떠한 입력 문장에 들어왔을 때 이제 그것은 멜룩키 커리로 구분되는데 이때 HG에 서로 다른 멜룩와 키 커리로 구분될 수 있도록 만드는 것입니다. 이렇게 주는 이유는 HG에 서로 다른 어연템션 컨셉을 학습하도록 만들어서 더욱더 구분된 다양한 특징들을 학습할 수 있도록 유도해 준다는 장점이 있습니다. 그래서 이와 같이 입력으로 들어온 값은 세계로 복제가 되어서 각각 멜룩키 커리로 들어가게 되고 이러한 멜룩키 커리값들은 리�니어 레이얼, 즉 행력업을 수행해서 HG로 구분된 각각의 코리상들을 만들어내게 되고 이때 여기에서 H는 헤드의 개수이기 때문에 각각 서로 다른 헤드길이 이렇게 멜룩키 커리의 싹을 받아서 어연템션을 수행해서 격할을 내보냅니다. 이제 그 다음에 앞서 말씀드렸듯이 이 어연템션 메커는 집에 입력 값과 이 출력 값의 디민전은 갓 해야 되기 때문에 이렇게 각각의 헤드로부터 나오게 된 어연템션 값들을 다시 이렇게 컴퓨셜 수행해서 일자로 쭉 붙인 뒤에 마지막으로 이 리�니어 레이어를 거쳐서 아웃북 값을 내보내게 됩니다. 이때 결과적으로 이 입력 값과 출력 값의 디민전이 갓도록 만들어서 이러한 멜룩키 헤드 어연템션 레이어를 사용한 뒤에도 디민전이 줄어들지 않도록 만듭니다. 바로 이런 식으로 각각의 어연템션 메커는 지금이 사용되는 것이고요. 이러한 멜룩키 헤드 어연템션은 이 전체 아케택처에서 다 동일한 함수로써 동작합니다. 이때 다른 점이라고 한다면 이렇게 사용되는 위치 마다 퀄리한 키랑 웨류를 어떻게 사용할지가 달라질 수 있는 건데 그런 점을 제외하고 기본적인 각각의 어연템션 레이어에 동작 방식은 같습니다. 그래서 예를 들어 이렇게 인코더 디코더 엇템션에서는 디코더의 출력 단어가 퀄리가 되는 것이고 각각의 출력 단어를 만들기 위해서 인코더 파티에서의 어떤 단어를 참고하면 좋은지를 구하기 위해서 이 키랑 웨르에 값으로는 인코더에 출력 값을 쓰겠다는 겁니다. 다시 말해 각각의 단어를 출력하기 위해서 어떤 정보를 참고해야 해 라고 이렇게 인코더한테 물어보는 것이기 때문에 이 디코더 파티에 있는 단어가 퀄리가 되고 인코더 파티에 있는 각각의 값들이 키와 웨륔가 된다고 할 수 있습니다. 그래서 멀테도 엇템션 레이어를 더욱 더 자세하게 수식으로 표현하면 다음과 같은데요. 자 이렇게 하나의 엇템션은 코리와 키와 멜류를 봤고요. 이때 코리랑 키랑 곡패에서 각 코리에 대해서 각각의 키에 대한 에너지 값을 구할 수 있겠죠. 이제 그런 에너지 값에 대해서 확륙 값으로 표현하도록 만들어서 실제로 어떤 키에 대해서 높은 가중치를 가지는지 계산할 수가 있고요. 이때 이렇게 스케일 세터로써 루트 디케일 사용합니다. 이때 디케이는 각각의 키 뒤민전이 되겠고요. 이렇게 특정한 스케일로 나눠주는 이유는 이 소프트 맥스암수 자체가 가지는 특성을 생각해 보시면 영 근처 위치에서는 그렬디언트가 높게 형성되는 것에 반해 값이 들충 날축 조금씩 왼쪽 오른쪽으로 이동하게 되면 기울개 값이 많이 줄어들기 때문에 그렬디언 웨니싱 문제를 피하기 위한 방법으로 이러한 스케일링 팩터로 넣어줄 수 있다고 논문해서 말하고 있습니다. 결과적으로 이렇게 각각의 코리가 각각의 키에 대해서 어떠한 가중치를 가지는지 스코어 값을 구한 뒤에 이제 개를 실수록 멜륵 값과 곡패서 오테션 멜류를 만들어낼 수가 있는 것입니다. 이때 말씀드렸듯이 입력으로 들어오는 각각의 값에 대해서 서로 다른 리�니얼 레이어를 거치도록 만들어서 H개 서로 다른 각각 코리 키 웨륵 값을 만들을 수 있도록 하는 것입니다. 이제 이런 식으로 H개 서로 다른 컨셉을 네트워크가 구분해서 확세파도록 만들무로써 오템션을 수영하기 위한 다양한 스쳐들을 확세파도록 만듭니다. 실제로 나중에 우리가 오템션 스코어를 시각화해볼 때는 이 H의 개수만큼 오템션 스코어의 그림이 나오게 됩니다. 이제 결과적으로 이렇게 각 H개에 대한 출력 값들을 구할 수 있고 이제 이것을 일자로 쭉 붙인 다음에 마지막으로 아 오픈 멜륵 세란 곡패서 결과적인 이 멀티헤드 오템션의 값을 구해낼 수가 있는 것입니다. 이런 식으로 매번 입력 값이 들어왔을 때 기본적으로 이런 식으로 멜륵와 키와 코리의 값으로 각각 들어와야 되고 이렇게 나올 때는 입력으로 들어왔던 값과 동일한 뒤면전을 가지기 때문에 이러한 멀티헤드 오템션 레이어가 포함된 하나의 인코더 혹은 디코덟 레이언은 중첩해서 사용될 수 있는 것입니다. 이제 한번 자세하게 이 트레스프모의 동작 올리를 알아보겠습니다. 지금은 그냥 간단하게 하나의 단어만 있다고 가정을 해볼게요. 이때, 오템션을 위해서 각각의 헤드 마다 코리와 키 멜륵 값을 만들 필요가 있습니다. 이제 그래서 이렇게 하나의 단어가 인배등 차원으로 표현되고 있는 상태에서 이제 여기에서 리�니얼 레이어를 거쳐서 각각 코리랑 키랑 멜륵 값을 만들 수 있습니다. 이때 인배등 차원을 딘 마덜리라고 부를 수 있고요. 원본 노몬에서는 인배등 차원을 512 차원으로 사용한다고 언급을 했고요. 이때, 만약에 헤드의 기술을 8개라고 하면 512를 8로 나는 64만큼 각각의 코리, 키 멜륵의 차원이 구성되는 것입니다. 여기 보이는 그림은 그냥 간단하게 인배등 차원이 4차원이고 헤드가 두 개라고 가정한 상황입니다. 즉, 이럴 때는 사고파기 2자리 메특스가 만들어지겠죠. 왜냐하면은 이 4차원의 데이터를 2차원의 데이터로 매피이 해야 되기 때문에 이렇게 사고파기 2차원의 가중치 메특스가 사용되는 것입니다. 그래서 이런 식으로 러브라는 단어가 4차원으로 표현되어 있다고 하면 이것은 코리, 키, 멜륵, 각각 2차원으로 구성되어 있는 데이터로 표현될 수 있는 것입니다. 이런 식으로 키랑 코리랑 멜륵을 다 구했다고 치면 바로 다음에 공식을 이용해서 실조로 오탄션 밸류를 구할 수가 있는데요. 이때, 코리는 각각의 다른 단어들 이 키와 행료급을 수행해서 이렇게 하나의 오탄션 에너지 값을 구할 수가 있는 겁니다. 예를 들어 ILOVE 유라고 하나의 문장에 들어왔다고 하면 이 I라는 단어는 I의 하나는 키, 러브의 당하는 키, U의 당하는 키값과 각각 고패져서 하나의 오탄션 에너지 값을 구할 수가 있는 거고요. 이 작가전에 말씀 드렸듯이 소프트 넥스에 들어가는 값의 크기를 노멜라이제이션 해주기 위해서 각각 스케일링 팩터로 나누어줍니다. 이제 이후에 소프트 넥스를 추회해서 실조로 각각의 키 값에 대해서 어떠한 가중치를 가지는지를 구해낼 수가 있는 것입니다. 여기에 보이는 그림에서는 이 I라는 단어는 I라는 단어와 72%만큼의 높은 영갈성을 가지고 ILOVE라는 단어와는 15% 그 다음에 유라는 단어와는 13% 이렇게 각각의 가중치를 가진다고 변할 수가 있는 거고요. 근데 이렇게 각각의 가중치 값에다가 이 멜룩값들을 각각 고팡 뒤에 전부 더웨져서 결과적인 오탄션 멜룩값을 만들어낼 수가 있는 것입니다. 즉 마찬가지로 웨이트 드넨성을 구할 수가 있다는 거고요. 바로 이러한 과정을 통해서 실조로 오탄션이 수행되는 것입니다. 그래서 한 번 실조로 전체 문장이 한꺼번에 입력되는 이런 행료과 같은 상황에서 예실을 다시 한번 확인해 보겠습니다. 실조로 이런 식으로 행료의 국세는 연사는 이용해서 한꺼번에 영상이 가능하고요. ILOVE 유라는 하나의 문장이 있고 그다음에 인배등 처음이 4천이라고 했을 때 바로 이렇게 3발의 사짜리 메테릭스로 구성되는데요. 이때 마찬가지로 하나의 헤드에 있는 이 코리키 멜료를 구하기 위한 가중치 값이 이렇게 있다고 해볼게요. 현재 헤드에서는 바로 이런 식으로 ILOVE 위에 대한 각각의 코리값, 킥값, 멜룩값이 만들어지는 것입니다. 마찬가지로 이렇게 코리아, 키와 멜룩값이 구해졌기 때문에 오탄션 멜료를 구할 수 있게 되는 건데요. 이렇게 ILOVE와 U, 코리값들을 한꺼번에 이렇게 각 키값과 고패해 줘서 오탄션 에너지를 이렇게 3발의 3브로 만들어낼 수 있습니다. 이때 오탄 에너지 값은 말 그대로 각각의 단어가 각각의 킥값에 대해서 얼마나 높은 그 연관성을 표현하는 수치를 부여했는지를 구할 수가 있는 겁니다. 즉, 이런 식으로 오탄션 에너지 값은 ILOVEU 각각에 대해서 구해지는 방식으로 이렇게 행각여랜 모두 단어에게 수와 동일한 표기를 가집니다. 각각의 단어가 서로에게 어떠한 연관성을 가지는지 구할 수가 있는 것이고요. 이제 여기에 소프트랙스를 취해서 각각의 행마다 각 키에 대한 값들을 확률값으로 구해낼 수 있도록 만드는 거고요. 이제 그러한 가중칫 값들과 밸력값을 고패해 줘서 실제 오탄션 밸려 메트릭스를 구할 수 있습니다. 보시면 이제 이렇게 오탄션 밸력값 자체는 입력되었던 코리와 키와 밸류와 모두 동일한 차원을 가집니다. 또한 한 가지 알아도시면 좋은 점은 마스크에 사용할 수 있다는 점인데요. 이 마스크에 있는 마스크 메트릭스는 특정한 단어를 무시할 수 있도록 하기 위해 사용할 수 있습니다. 이렇게 오탄션 에너지 값이 있을 때 오탄션 에너지와 같은 차원에 마스크 메트릭스를 만들어서 이제 얘를 엘러먼트 와이즈로 즉각각의 원소 단위로 고패해 주어서 어떠한 단어는 참고하지 안토록 만들 수가 있는 것입니다. 예를 들어서 이렇게 이 아이라는 단어는 이 러브와 유의 해당하는 키값은 무시하도록 즉 이 러브와 유는 그냥 오탄션 하지 안토록 무시하고자 한다면 이렇게 오탄션 에너지 값을 전부 다 마이너스 무한이라고 할 수 있는 가능한 초대로 작은 값을 넣어주게 되면 실제로 소프트는학스를 취해서 오탄션 스코어 값이 구해졌을 때 고려하지 않도록 처리가 된 그런 단어들에 대해서는 모두 0%에 가둥치 값을 가지게 됩니다. 즉 마스크 메트릭스를 그냥 쉬워 주물로서 특정한 단어는 무시해서 오탄션을 수행하지 안토록 만들 수가 있는 것입니다. 이와 같이 마스크 메트릭스를 이용해서 이런 오탄션 에너지 값에 마스크를 적용함으로써 특정 단어는 그냥 무시해서 오탄션을 수행하지 안토록 만들 수가 있는 것입니다. 그래서 결과적으로 이렇게 각각의 헤드 마다 입력으로 들어온 코리아 키와 멜류와 같은 차원의 백터를 만들어내기 때문에 이렇게 각 헤드 마다 코리아 키와 멜류의 값들을 각각 넣어서 오탄션을 수행한 값들을 이렇게 다 헤드월부터 헤드 H까지라고 했을 때 이러한 정보들을 다 일자로 쭉 연결하게 되면 다시 맨처에 입력이 되었던 이런 입력 디밴 전과 같은 디밴 전을 가지게 되는데요. 다시 말해 이런 식으로 멀케이드 오탄션은 각각의 헤드에 대해서 오탄션을 수행한 뒤에 그러한 결과를 다시 쭉 이어 붙이기 때문에 결과적으로 만들어진 메트릭스에 이 열의 갯수는 원래 입력에 임배 등 차원과 동일한 값을 가집니다. 그렇기 때문에 이제 마지막에 EW 가중치 값으로 D 모델 고파기 D 모델 차원을 가지는 메트릭스를 고패 주물로써 결과적인 멀케이드 오탄션의 값을 구할 수 있고요. 이제 이렇게 하더라도 결과 값은 입력 디밴 전과 정확히 동일하기 때문에 이러한 멀케이드 오탄션을 수행한 기회도 차원이 동일하게 유지가 된다는 점이 특징입니다. 또는 앞서 간단하게 말씀 드렸듯이 이 트레스 폰호에는 세 가지 종류의 오탄션이 사용되는데요. 트레스 폰호의 쓰이는 오탄션은 항상 멀케이드 오탄션으로 헤드가 여러 개의 오탄션이라고 볼 수 있는데 이제 그런 오탄션이 사용되는 위치에 따라서 인코더 셀프 오탄션 그리고 마스트 디코더 셀프 오탄션 인코더 디코더 오탄션 이 세 가지 종류가 존재합니다. 기본적으로 인코더의 셀프 오탄션은 말씀 드렸듯이 각각의 당황과 서로에게 어떠한 연관성을 가지는지를 오탄션을 통해서 구하도록 만들고 전체 문장에 대한 레프레젠테이션는 널링할 수 있도록 만든다는 점이 특징이고요. 다만 이제 디코더 파트에서 셀프 오탄션을 수행할 때는 이렇게 각각의 출력 단어가 다른 모든 출력 단어를 전부 참가도록 만들지는 않고 앞쪽에 등장했던 단어들만 참고할 수 있도록 만듭니다. 예를 들어 출력 문장이 나는 축구를 했다 라고 하면 우리가 축구를 일하는 단어를 출력할 때 있어서 했다라고 이렇게 뒤쪽에 나오는 단어가 무엇인지 참고할 수 있도록 만들어버리면 그것은 이제 일종의 치팅처럼 동작을 하기 때문에 모델이 정상적으로 학습이 되기가 어렵습니다. 그렇기 때문에 이 디코더 파트에서 오탄션을 수행할 때는 이렇게 각각의 단어에 대해서 이 앞쪽 단어들만 참고할 수 있도록 만들모로써 치팅을 하지 않고 정상적으로 모델이 학습될 수 있도록 만드는 것입니다. 마지막으로 인코더 디코더 오탄션은 커리가 디코더에 있고 각각의 키와 밸려는 인코더에 있는 상황을 의미하는 것입니다. 예를 들어 난 널 좋아해라고 아예 라이크 유라고 문장이 들어왔을 때 출력 문장이 난 널 좋아해라고 나온다고 하면 각각의 출력 단어들이 이러한 입력 단어들 중에서 어떤 정부에 덕더 많은 가중치를 두는지 구할 수 있어야 되는데요. 이제 그러한 가정에서 이 디코더 파트에 있는 코리 값이 이렇게 인코더 파트에 있는 키와 밸려 값을 참조한다고 해서 인코더 디코더 오탄션이라고 부르는 것입니다. 또한 이어서 셀프 오탄션에 대해서 알아볼 건데요. 실제로 이러한 셀프 오탄션은 말씀 드렸듯이 인코더와 디코더 모두에서 사용되고요. 시각화 가정을 통해서 오탄션 스코어로 나온 값을 그려볼 수 있습니다. 매번 입력 문장에서 각 단어가 다른 어떤 단어가 연괄성이 높은지를 구할 수가 있는 건데요. 예를 들어 이런 식으로 하나의 입력 문장이 들어왔을 때 각각의 단어들은 다른 모든 단어에 대해서 오탄션 스코어 값을 구할 수가 있는 겁니다. 예를 들어 이렇게 모이 후이 슬루킹에 더 추위 이제서 프로라이즈 이런 식으로 문장이 있다고 했을 때 각각의 단어들은 다른 단어 모두에 대해서 얼마나 가중치를 부여할지를 오탄션을 통해서 계산할 수가 있는 건데요. 예를 들어 이렇게 이질한 단어를 출력한다고 하면 이런 이식 의미하는 단어는 앞쪽에 있는 트리와 이렇게 동일한 이식 되겠죠. 그렇기 때문에 실제로 오탄션 스코어를 시각적으로 출력하도록 만들면 이런 식으로 트리와 입과 관련해서 더 높은 스코어를 가지는 방식으로 학습이 될 가능성이 높습니다. 그래서 이런 식으로 각각의 단어들이 서로 어떠한 연관성을 가지는지를 셀프 오탄션 과정을 통해서 시각화 해볼 수가 있습니다. 자 이제 결과적으로 우리가 앞에서 확인했던 이 트레스フォ魔의 전체 아켓택처에 포함되어 있는 내용들을 하나씩 확인해 보았는데요. 이렇게 인코더 파트에서 인력까지 들어와서 위치에 대한 정보를 반영해 준 입력을 실제로 첫 번째 레이어 넣어주게 되고요. 이제 이렇게 인코더 레이어는 앤번만큼 반복이 되어서 중접해 사용이 되고 이제 그렇게 나온 마지막 레이어의 인코더의 출력 값이 각각의 디코더 레이어에 들어간다고 보시면 됩니다. 이제 그래서 마찬가지로 디코더 레이어도 앤번만큼 중접이 되어서 가장 마지막에 나온 그 출력 값에 리니얼 레이어와 소프트 맥스를 취해서 각각의 출력 단어를 만들어낼 수가 있는 것입니다. 다만 이제 우리가 한 가지 얘기 안 한 게 있다고 하면 바로 위치 정보를 어떤 식으로 넣을지에 대한 인코딩 함수입니다. 원본 논문에서는 하나의 문장에 포함되어 있는 각각의 단어들에 대한 상대적인 위치에 대한 정보를 모델에게 알려주기 위해서 바로 주기함수를 활용한 공식을 사용하는데요. 실제 공식은 바로 다운가 같습니다. 이때 P인은 포지션을 인코딩에 약자고요. 이때 이 포스는 각각의 단어 번호가 되겠고요. 이때 이 I는 각각의 단어에 대한 인배된 값의 위치 하나하나를 의미합니다. 이제 그래서 이런 식으로 싸인함수와 같은 주기함수값을 인코딩을 위해서 사용하는데요. 이렇게 8m로 들어와 있는 만과 같은 값이나 이런 싸인과 코사의 남수는 이렇게 기본적인 싸인함수와 코사의 남수 말고 다른 주기함수를 사용할 수도 있는 거고요. 아무튼 우리 네트워크가 각각의 입력 문장에 포함되어 있는 각 단어들에 상대적인 위치에 대한 정보를 알 수 있도록 이런 주기성을 학습할 수 있도록 만들기만 한다면 어떤 함수가 들어와도 사용할 수 있습니다. 그래서 원본 논문에서도 이렇게 싸인함수와 코사의 남수를 이용해서 정해진 그런 함수값에 사용할 수도 있지만 우리가 위치에 대한 임배딩 값을 다루 학습하도록 만들어서 네트워크에 넣을 수 있다고 말하고 있고 실제로 그렇게 넣었을 때도 이렇게 싸인함수와 코사의 남수를 이용했을 때와 실제 성능상인 차이는 거의 없었다고 말하고 있습니다. 그래서 실제로 트레스프머 이후에 나온 다양한 아키텍처에서는 이러한 주기함수를 사용하지 않고 그냥 학습이 가능한 형태로 별도의 임배딩 레이어를 사용하기도 합니다. 더욱더 자세하게 실제로 이러한 위치인코딩이 어떤 식으로 들어갈 수 있는지를 확인해 보시면 예를 들어 이렇게 입력 문장이 위치워드 원이라고 한번 해볼게요. 이때 각각의 단어들은 딘 마다에만큼의 임배딩 처론을 가지게 됩니다. 지금 그림에서는 이 임배딩 처론이 8이 되겠죠. 이 싸인과 코사의 남수에 들어가는 이 포스값과 아이값은 이러한 입력 행료 값에서의 각각의 인득스값과 동일하게 들어가는 것입니다. 예를 들어 여기는 0컨마 3이 되는 데요. 첫 번째 단어의 네 번째 임배딩이기 때문이죠. 그래서 이제 각각의 값들이 이러한 함수에 들어가게 되면서 바로 입력값과 정확히 동일한 디반전을 가지는 위치인코딩을 만들어낼 수 있습니다. 그래서 이제 이 값을 엘리먼트 와이지로 다 더해 줘서 원소바이 원소로 다 더해준 뒤에 그 값을 실제로 각 인코드와 디코더레이어의 입력값으로 사용을 한다고 보시면 되겠습니다. 여기 보이는 코드는 그냥 간단하게 앤과 디반전에 대해서 실제로 어떤 식으로 각 단어의 위치에 대한 인코딩 정보가 들어가는지를 그림으로 표현한 것인데요. 바로 이렇게 간단하게 매플럴 라이브로이를 이용해서 그림을 그려볼 수 있습니다. 자 마찬가지로 전체 실수 코드는 제 기탑 저장소에 올려놓았으니까요. 확인하실 수 있습니다. 이렇게 아래쪽에 내려야 보시면 얻은 션이스 올류내드 코드 프랙티스 보시죠. 여기 들어와서 전체 코드를 확인해 보실 수 있습니다. 전체 코드는 여러분들의 개인 개발한경이 아닌 무료 딘러닝 개발한경인 코랩에서 바로 실행해 볼 수 있도록 준비를 해놓았습니다. 그래서 여러분들은 구글 아이디만 있으시면 바로 여기 링크 들어와서 코랩에서 즉시 실행해 보실 수 있습니다. 다음과 같이 전체 코드를 확인해 볼 수 있는데요. 내용을 확인해 보시면 기본적으로 본 코드는 트랜스 포머의 논문 내용을 최대한 따르면서 구현된 코드입니다. 실제로 트랜스 포머 논문은 디렁닝 개발의 자연스러운 기법에 대표적인 기본적인 구성을 이해하고 공부하는 데에 도움을 줍니다. 그래서 최근 가장 뛰어난 번영 모델들은 이러한 트랜스 포머 기관의 아키텍처를 따르는 경우가 많습니다. 코드를 실행하시기 전에 일원타임에서 런타임 유용 변경에 들어오신 뒤에 GPU로 서정에 되어 있는지 확인해 주세요. 또한 본 코드에서는 독일어를 영어로 번역을 쓰행해 볼 건데요. 이제 번역된 영어 문장의 성능을 평가하기 위한 척도로 블루스코어를 사용할 예정입니다. 이 블루스코어는 엔그램 기반으로 번역한 문장이 실제 정답 문장들과 비교했을 때 얼마나 유사한 지를 평가해 주는 평가 척도 중 하나입니다. 그래서 이걸 사용하기 위해서 토지 텍스티를 특정 버전으로 설치할 수 있도록 만들어주겠습니다. 이제 이어서 데이터의 전처리를 진행하게 될 건데요. 문장의 토크 날을 진행할 겁니다. 이때 우리는 도비로를 영어로 번역하는 데스크를 진행할 것이기 때문에 이렇게 영어와 독일어에 대해서 전처리 모질을 설치할 수 있도록 하겠습니다. 그래서 각각 영어와 독일어에 대해서 어떤 문장이 있을 때 그런 문장을 토크 누로 바꿔주기 위해서 토크 누로 바꿔줄 수 있는 각각의 스페셜 라이브로리 객체를 선언해 주고요. 한 번 간단하게 I.M.O. 브레즈의 student, 라는 내용이 문장이 있을 때 한 번 이걸 토크 누로 각각 바꿔서 출력을 해 보겠습니다. 자 그럼 이런 식으로 영어 문장이 정상적으로 I.M.O. 미레즈의 student을 하고 잘 토크 날의지가 된 걸 확인할 수 있습니다. 이제 우리는 실제로 다수의 문장을 불러와서 강 문장마다 전부다 이러한 토크 날을 진행해서 우리 네트�워크의 입력 값으로 넣을 수 있도록 만들 겁니다. 그렇기 때문에 도비로와 영어 각각에 대해서 어떤 문장이 들어왔을 때 토크 날을 수용한 결과를 다시 리스트 형을 대로 바란할 수 있도록 함수를 정리합니다. 저렇게 마찬가지로 함수를 정리해 주시고 또 추가적으로 필드 라이브로리를 이용해서 어떠한 데이터셋이 있을 때 그러한 데이터셋에 대해서 어떻게 전처리를 수행할 건지 명시할 수 있도록 합니다. 번영 모델의 입력을 넣을 때는 각각의 문장들의 앞부분에는 SOS 토크를 붙이고 가장 뒷부분에는 US 토크를 붙이는 것이 일반적입니다. 또한 각각의 단어들은 모든 소문자로 바꿔주는 것이 일반적이고요. 또한 트레인스 부모의 입력을 넣을 때는 댄서의 차원에서 시코스보다는 배치가 먼저 오도록 만드는 경우가 많기 때문에 이렇게 배치4ST의 값으로는 트로크 값을 넣어 주겠습니다. 이제 이어서 약 3만 대 정도에 영어 도거 쌍을 가지고 있는 번역 데이터셋인 멀티설티케일을 불러와서 데이터를 초기할 수 있도록 해줄게요. 이때 각각 이 필드 라이브로리를 이용해서 독일어를 영어로 바꾸는 테스트 퀴에도 해서 각각 앞서 정리했던 전처리를 수행할 수 있도록 하는 것입니다. 이제 약습 데이터셋과 평가 데이터셋 테스트 데이터에 대해서 개수를 출력하도록 만들어보시면 이와 같이 약습 데이터가 29000개 그리고 평가 데이터와 테스트 데이터가 각각 약 1000개 정도 문장으로 구성되고 있는 걸 확인할 수 있고요. 이때 한번 간단하게 인데X 30번에 해당하는 학습 문장을 출력하도록 만들어보시면 바로 이러한 독일어 문장에 들어왔을 때 이러한 영어 문장을 출력하도록 학습 데이터가 구성되고 있는 걸 확인할 수 있고요. 자, 그래서 이제 실제로 무케블러리 세트를 만들 수 있습니다. 이 무케블러리를 만들어주는 이유는 독일어를 영어로 번역할 때 각각의 초기 임프티면 전에 얼마인지를 구할 수가 있기 때문입니다. 그래서 전체 단어들 중에서 최소 2번 이상 등장한 단어들만을 선택하도록 만들어서 무케블러리 세트를 만된 뒤에 각각의 렉스를 출력하도록 만들어보시면 독일어는 785개의 유임이한 단어가 있고 그리고 영어는 5893개 각각의 단어들이 존재한다고 볼 수 있는 거예요. 그래서 이런 무케블러리 객체에서 스트링2 아이 함수를 호출해 가지고 각각의 단어가 어떤 인데X에 당하는지를 구해 볼 수 있습니다. 예를 들어 영어에서 이렇게 ABC, ABC라는 단어가 존재하는지 확인할 수 있는데요. 이때 만약에 이런 식으로 애초에 등장하지 않았던 없는 단어라면 영이라고 뱉는 걸 확인할 수 있고요. 이렇게 아이 의미가 없는 그런 공간에 당하는 패딩 값은 일로 들어가 있는 걸 확인할 수 있고요. 저렇게 SOS와 EOS는 기본적으로 2번과 3번에 당합니다. 또한 이렇게 실제로 존재하는 단어 같은 경우는 그 단어의 인데X가 나오는 걸 확인할 수 있고요. 이제 2화같이 앞쪽에 붙는 4개의 토큰들이 차례대로 언노운 토큰 그 다음 패딩 토큰 SOS 토큰 EOS 토큰이라고 불리고요. 이 4개의 토큰들은 실제로 존재하는 단어는 아니지만 네토어 그가 각각의 문장들을 적절하게 학습할 수 있도록 만들기 위해 사용하는 토큰들입니다. 이어서 한 문장에 포함된 단어들이 순서대로 날인 상태로 네토어 그 입력이 되는데요. 이때 하나의 배치에 포함된 문장들이 가지는 단어의 갯수가 유사할 수 있도록 만들기 위해 버킷이터레터를 사용할 수 있습니다. 이때 이런 식으로 배치 사이즈를 정해질 수 있는데요. 하나의 배치에 포함되어 있는 각각의 문장들의 그 시컨스 랭스가 가능한 유사도로 만들어서 길이가 짧은 문장들에 대해서 패딩토큰이 최대한 적게 들어갈 수 있도록 하여 실제로 학습을 위해 네토어 그 입력으로 들어가는 데이터에 차원을 줄일 수 있습니다. 그래서 이렇게 각각 학습용, 평가용, 테스트용 데이터 세셀 이터레이터로 만들어주고요. 한 번 간단하게 이 트렴이터레이터에 포함되어 있는 첫 번째 배치를 확인한 뒤에 하나의 문장에 대한 정보를 출력해 볼 수 있습니다. 자, 확인해 보시면 현재 배치에 포함되어 있는 문장들 중에서 가장 긴 문장의 시컨스 길이가 35가 되겠고요. 현재 출력한 문장은 이 배치에 포함되어 있는 첫 번째 문장이고요. 이런 식으로 어떤 문장이 들어가 있는데 여기 이제 이번 같은 경우는 SOS가 되고 이 3번은 EOS라고 했죠. 그래서 이제 SOS와 EOS 안에 포함되어 있는 각각의 데이터가 실제 하나하나씩 단호를 의미하는 거고요. 이제 이렇게 EOS가 나온 뒤에는 뒤쪽에 다 패딩토큰이 붙어가지고 의미가 없는 토크니라는 것을 알려줄 수 있도록 하는 것입니다. 이제 이런 식으로 현재 배치에 있는 하나의 문장에 포함된 정보를 출력할 수 있습니다. 이때 이 값은 시컨스 랭스가 되기 때문에 우리는 첫 번째 문장에 있는 각각의 단호들을 출력하도록 만들 수가 있는 것입니다. 이제 그래서 실제로 트리스프모형 논문과 최대한 유상하게 각각의 아키텍처를 정리해서 모델 학습할 수가 있는데요. 가장 먼저 멀티해도 엇텐션입니다. 확인해보시면 이렇게 엇텐션의 세 가지 옷으로 입력으로 봤고요. 커리와 키와 멜류입니다. 기본적으로 커리 키 멜류의 차원들은 모두 뒤인 모델을 H로 나는 값으로 차원을 모두 갓도록 만들면 간단하게 정리할 수 있습니다. 이렇게 8m로 세계를 봤는데 먼저 히든 디밤전은 하나의 단호에 대한 인베딩이 되겠고요. 이때 N-H는 말 그대로 H가 되겠습니다. 헤드의 개수가 되는 거고 그다음에 드라바울레이시우는 별도의 정규와 테크닉으로 드라바워서 적용할 때의 비율을 설정하는 것입니다. 참고를 여기에서 우리가 앞서 공부했을 때는 각각의 커리와 키 멜류들은 히든 디밤전에서 이 키의 차원으로 바뀌어진다고 말씀을 드렸는데요. 실제로 구현할 때는 이와 같이 그냥 히든 디밤전을 히든 디밤전으로 메이핑하도록 만든 다음에 이 결과 디밤전을 H로 쪽에서 사용할 수 있는 겁니다. 자, 그래서 확인해 보시면 각 헤드의 포함되어 있는 그 인베딩 차원은 헤드 디밤전이라고 해서 이 단어들의 인베딩 차원을 H로 나는 값을 사용하도록 만든 겁니다. 또한 스킬값 또 앞서 설명했던 내용과 마찬가지로 각각의 커리아 키와 멜류들에 당하는 그 값의 루틀을 시원 값을 나중에 나눠주어서 소프트 맥스에 넣어줄 수 있도록 만든 겁니다. 그래서 이때 각각 커리아 키 멜류들 오는데요. 이때 커리 뱅스는 단호의 개수가 되겠죠. 그래서 이와 같이 각각 커리아 키 멜류로 그대로 다 맵핑을 해주고요. 이때 여기에서 우리는 이러한 커리아 키의 멜류에 별값값들을 H로 나눠 사용할 수 있는 것입니다. 즉, H게 각각마다 헤드 디밤전만 금액 크기로 차원을 가지도록 만들어서 앞서 확인했던 그림에서 이렇게 리�니어를 각각 거친 값을 뽑아낼 수가 있는 것입니다. 이제 그래서 각각 H게, 멜류들, 키, 커리아 키 만든 것과 마찬가지입니다. 이제 그렇게 나눠오진 상태에서 각각의 헤드마다 커리아 키을 서로 고파드로 만들어주고 스케일로 나눠줍니다. 그래서 이렇게 에너지를 구한 다음에 필요하다면 마스크를 쉬울 수 있다고 했었죠. 이때 마스크 값이 0인 부분을 전부 다 마이너스 무한의 각각 값으로 넣어 주어서 실제로 소프트 맥스에 들어간 격각 값이 거의 0%가 될 수 있도록 만드는 거고요. 이제 그래서 소프트 맥스를 취한 다음에 그렇게 나오게 된 어차션 가중칩 값들을 실제로 V 값과 곱해서 어차션 멜류 값들을 결과적으로 만들어줄 수가 있는 거고요. 이제 얘를 다시 일자로 쭉 늘어뜨려서 컴퀴즈 수행한 것과 동일한 결과를 뽑을 수 있도록 만듭니다. 그래서 결과적으로 마지막에 아웃은 리녀암수를 거쳐서 결과를 뽑아내고 그리고 어차션 스쿼가 압센 따로 또 출력하도록 만들어서 나중에 시각하를 사용할 수 있도록 만듭니다. 이어서 포인트 와이즈 휴드4 라키 텍션인데요. 마찬가지로 이렇게 휴드인 디밤점만 금의 차원이 들어왔을 때 휴드인 디밤전료를 그대로 내버린다는 점이 특징이고요. 즉 입력과 출력의 차원이 동일합니다. 이제 이어서 앞에 정의했던 어차션과 휴드4들 레이어에 대한 클래스를 이용해서 실제로 하나의 인코덟 레이어에 아키텍처를 정의할 수가 있는데요. 마찬가지로 입력과 출력의 차원이 갔고요. 그 다음에 이제 이렇게 정의된 인코덟 레이어를 실제로는 여러 번 중첩해서 하나의 전체 인코덴에 들어가게 됩니다. 자, 그래서 인코덟 레이어를 확인해 보시면 내게에 실질적인 레이어가 들어가 있는 걸 확인할 수 있는데요. 여기 그림을 확인해 보시면 실질 인코덟 레이어에는 말씀드렸듯이 어차션 이후에 레지드와 커뮤니티션과 노멜라이저이션 그 다음에 휴드4 레이어에 다시 한번 레지드와 커뮤니티션과 노멜라이저이션 수행을 한다고 말씀을 드렸죠. 그래서 각각의 레이어가 이렇게 내게 차려들어 들어가 있는 거고요. 가장 먼저 하나의 입력 값이 들어왔을 때 그 입력 값을 커리랑 키 밸류로 그대로 복제해서 같은 값들을 넣어지도록 만들고요. 우리가 특정 단어에 대해서는 어차션을 수행하지 않도록 하기 위해서 마스크 백터를 쉬울 수 있다고 했습니다. 그대로 건너온 입력 임베딩 매트릭스와 마스크를 그대로 넣을 수 있도록 하는 거고요. 그래서 가장 먼저 이렇게 셀프 어차션을 수행해서 벽할 뽑아는 뒤에 구회전 값을 더해 줘서 레지들어 커뮤니티션을 수행한 뒤에 실제로 노멜라이저이션을 수행한 결과가 나올 수 있도록 하고요. 이제 마찬가지로 FIDFORD 레이어를 거친 다음에 다시 한번 레지들어 커뮤니티션을 거쳐서 만들어진 아웃푸스를 내보내는 걸 확인할 수 있습니다. 그래서 실제 인코드 아케택처에서는 이렇게 앞서 정의했던 인코드 레이어를 총 N 레이어만큼 반복해서 썼도록 만듭니다. 확인해보시면 인코드 파트에서는 실제 단어들의 객수에 당하는 인프 디멘전에에 당하는 매트릭스가 들어왔을 때 전부 다 임베딩 차원으로 바꿔주고요. 그 다음 또 추가적으로 여기에 해서 바로 이 부분이 실제 노무인과는 다르게 구연된 부분이라고 할 수 있습니다. 현재 코드에서는 원본 노무인과 다르게 위치 인배딩을 직접 학습하는 형태로 구현합니다. 즉 특정 파라미터로 이미 정해져 있는 쌍과 코사애람스를 사용하는 것이 아니라 이런 위치 인배딩을 학습하도록 만들어도 비슷한 결과를 내보낼 수 있기 때문에 본 코드에서는 이렇게 인배딩 값을 학습하는 레이어로써 별도의 학습 레이어로써 사용할 수 있다고 보여주는 것입니다. 또한 이제 실조로 마스크 값은 이 패딩토크는 데에서 영값이 들어간 형태로 만들어지는데요. 말씀드렸듯이 하나의 배치 안에는 여러 개 입력 문장에 들어가 있는데 이때 짧은 문장에 대해서는 뒤쪽에 패딩토크으로 채워진다고 했습니다. 그렇기 때문에 패딩토크는 데에서는 그 문장에 포함되어 있는 단어절길이 어색션을 수행할 때 패딩토크는 무시하도록 만들어야 하기 때문에 이렇게 패딩토크는 데에서 마스크를 씌워준다고 보시면 되겠습니다. 자 그래서 먼저 맨 처음에 인프티 맨전으로 입력에 들어왔을 때 실제 인배딩 차원으로 바꿔주고 거기다가 위치에 대한 전복 값을 더해지도록 만들기 위해서 별도의 포스 인배딩 레이어를 추가해 준 거고요. 이제 인코덟 레이어연은 반복적으로 중첩되어서 사용이 되기 때문에 모를리스트를 이용해서 N 레이어만큼 반복할 수 있도록 만듭니다. 그래서 처음에 입력이 들어왔을 때 이 배치 사이즈는 말 그대로 문장의 갯수가 되겠고요. 이 소스 랭스는 강 문장되어 중요해서 단어에 갯수가 가장 많은 문장에 단어 갯수가 되겠습니다. 이제 그래서 포리처를 인코딩은 차례대로 영부터 가장 긴 문장에 당하는 번호까지 들어갈 수 있도록 만들고 이제 그것을 각각의 문장마다 적용하도록 만들기 위해서 리필을 수행합니다. 그래서 결과적으로 입력 인배딩 값에 그런 위치에 대한 전복가 포함된 데이터를 실제 입력 값으로 사용을 해주는 거고요. 그래서 그러한 입력 값이 여러 개의 레이어를 반복적으로 거치면서 순정파 직포올들을 수행할 수 있도록 만듭니다. 마지막 인코덟 레이어에서 나오게 된 그 출력 값을 결과적으로 사용할 수 있도록 만드는 것입니다. 이어서 디코덟 레이어에 아키텍처를 확인해 보겠습니다. 마찬가지로 입력과 출력의 차원이 갔고요. 그래서 트리스포모의 디코덟은 이러한 디코덟 레이어를 여러 번 중첩해서 사용한다고 말씀드렸죠. 또한 내부적으로 두 개의 멀케이드 엎텐션이 사용된다고 말씀드렸습니다. 하나는 셀프 엎텐션이고요. 그리고 하나는 인코덟에 대한 정보를 받아오기 위한 인코덟 디코덟 엎텐션이라고 말씀드렸습니다. 그래서 내용을 확인해 보시면 총 6개의 레이어가 사용되는 걸 알 수 있고요. 이는 여기 그림과 마찬가지입니다. 셀프 엎텐션 이후에 레지돌 컨랙션 그 다음에 인코덟 디코덟 엎텐션 이후에 레지돌 컨랙션 이후에 휴드4드 레이어와 레지돌 컨랙션을 한 번 더 거쳐서 결과는 뽑아낼 수 있도록 만듭니다. 이제 이런 레이어가 앤번만큼 중첩되어 쓰이고요. 그렇게 마지막 레이어에서 나오게 된 출력값이 실제 출력 문장에 대한 정보를 가지고 있다고 했죠. 따라서 이 와가 6가지 레이어를 전부 초기할 수 있도록 만들어주었고요. 이제 이렇게 가장 먼저 셀프 엎텐션을 사용합니다. 그렇기 때문에 코리와 키 밸류는 모두 자기 자신을 넣을 수 있도록 만들어주고요. 그 다음 이후에 레지돌 컨랙션을 수행한 뒤에 그 다음에 인코덴에서 정보를 가져오는 인코덟 디코덟 텐션을 사용하는데요. 이때 코리는 바로 디코덟에 포함되어 있는 출력 단어들에 대한 정보가 되겠고요. 인코덴에서 가장 마지막 출력값으로 나온 그 값을 키로 사용하는 것을 알 수 있습니다. 그래서 결과적으로 이렇게 정의된 디코덟 레이어와 아켓택처를 이용해서 전체 디코덟 아켓택처에서는 그러한 디코덟 레이어를 여러 번 중첩해서 사용하고요. 이제 원본 너무나 다른 게 마찬가지로 위치인 베딩을 우리가 벨둘 확세판을 형태로 구현을 해서 이 싸인 암스와 콧사애람스를 사용하지 안토록 만들 것입니다. 그치 타겐 문장에서 가각에 단어는 다음 단어가 무엇인지 알 수 없도록 즉 이전에 출력한 단어만 볼 수 있도록 하기 위해서 벨둘의 마스크 벡터를 사용할 수 있다고 했습니다. 그래서 아까랑 동일하게 단어의 개수와 같은 그 디메전을 인배등 차원으로 바꾸워 죽어요. 그다 마찬가지로 위치에 대한 정보를 주기 위해서 정체 시코스의 랭스에 당하는 디메전을 히든 디메전으로 바꿀 수 있도록 만듭니다. 또한 이어서 이러한 디코덟 레이어와 반복적으로 중첩해 사용할 수 있다고 말씀드렸습니다. 그래서 실제로 포함들을 수행할 때 인코덟의 마지막 레이어에서 나오게 된 출력값과 실제로 타겐 문장에 대한 정보를 바꾸요. 마찬가지로 타겐 문장 또한 즉 출력하기 위한 문장 또한 영부터 단어의 개수까지에 대한 위치에 대한 정보가 담겨야 하기 때문에 하나의 텐서를 초기화한 뒤에 강 문장에 대해서 모두 동일하게 적용할 수 있도록 만듭니다. 그래서 문장이 인배등 값에 위치에 대한 정보를 더한 것을 실제 입력으로 사용을 하고요. 그래서 디코덟 레이어를 열어본 거친 다음에 마지막에 나오게 된 아웃붙 값에 출력을 위한 리니얼 레이어를 거치도록 만들어서 결과적인 아웃붙 값을 뽑아낼 수 있습니다. 그래서 결과적인 트레스 부모 아키텍션은 이렇게 전체 인코덟 아키텍처와 전체 디코덟 아키텍처를 받아서 입력 문장에 따라서 마스크를 붙여서 실제 결과를 뽑아낼 수 있도록 만듭니다. 이때 소스 문장과 등계온은 패딩토크는 대해서만 마스크 값을 영어로 설정해서 학습하도록 만들고요. 이때 타겐 문장 또한 마찬가지로 우리가 학습을 진행하는 과정에서 다음 단어가 무엇인지 알 수 없도록 하기 위해서 마찬가지로 마스크 행료를 사용할 수 있습니다. 여기 보이는 패딩 마스크는 소스 문장과 마찬가지로 패딩토크는 대해서만 적용하도록 만들어서 학습할 때 이러한 마스크를 사용할 수 있도록 하고요. 추가적으로 앞쪽에 있는 단어들만 볼 수 있도록 만들기 위해서 별도에 마스크를 하나도 만든 다음에 이 두 마스크에 대해서 엘러먼트 YG로 ND 영상을 진행한 뒤에 결과적으로 이렇게 두 마스크에 대해서 둘 다 1일 값을 가지는 그런 위치에 대해서만 실제 어템션 스코어를 구할 수 있도록 만들어서 이렇게 마스크 값을 생성하는 것을 알 수 있습니다. 그래서 결과적으로 소스와 타겟이 들어왔을 때 마스크를 각각 만든 뒤에 먼저 인코드에 이런 소스 문장을 넣어서 인코드에 출력값들을 뽑은 뒤에 이제 뒷코드는 매번 그런 인코드에 출력값을 얻은 수 있도록 만들어서 결과적으로 마지막에 나온 아웃푹값이 우리 네트워크에 번역 결과라고 할 수 있습니다. 이제 그래서 시저로 학습을 진행할 수 있는데요. 학습할 땐 이와 같이 인프 띄는 점과 아웃푸 띄는 점은 각각 소스 언어에 포함되어 있는 언어의 개수 그다음에 타겟 언어에 포함되어 있는 언어의 개수가 될 수 있도록 합니다. 그래서 각각에 단어에 대한 인배딩 차원은 256으로 설정하고 레이가 총 3번씩 중첩되어 사용할 수 있도록 만들었습니다. 여기에 보이는 파라미터는 실제 논문에서 제한된 파라미터에 비하면 크기가 많이 작은 편이지만 그럼에도 불구하고 충분히 좋은 성능을 낼 수 있습니다. 그래서 결과적으로 이렇게 트린스폼어 객체를 만들어준 뒤에 파라미터까지 다 조교할의 진행해 주시고요. 이렇게 전체 네트워크에 포함되어 있는 모든 파라미터를 확인해 볼 수 있습니다. 그래서 이제 한 번 학습을 진행할 수 있는데요. 이와 같이 런닝레이트는 0.0 0 0 5로 하고 에덤 옵티마이저를 사용해서 모델 학습할 수 있도록 하겠습니다. 그래서 학습 함수와 평가 함수를 따로 정의해서 학습을 진행하면요. 자 이렇게 학습이 진행되는 걸 확인할 수 있고요. 학습을 진행할 때마다 멜리데이션 로스가 더 감소하는 경우에만 모델 파라미터를 새로운 파일로 기록하도록 만드는 것을 알 수 있습니다. 결과적으로 이와 같이 학습이 완료된 걸 확인할 수 있고요. 이렇게 학습된 결과를 여러분들의 컴퓨터에 기록하고자 한다면 이렇게 파일 라이브로를 이용해서 학습이 완료된 데이터를 다울로도 받을 수 있도록 합니다. 자 이후에 이와 같이 학습 완료된 모델을 이용해서 테스트 데이터에 대해서 이별려의 현행 진행하게 되면 단가 같이 테스트 데이터에 대한 로스 값 구해볼 수 있습니다. 이제 이어서 여러분들만의 문장을 넣어서 실제로 우리 모델을 사용해볼 수 있는데요. 이렇게 트랜슬러의 센텐스 함수 안에 내용이 정의되어 있는 걸 알 수 있습니다. 먼저 하나의 문장이 들어왔을 때 토크너를 진행한 뒤에 앞 뒤로 SOS와 EOS 토크를 붙입니다. 각각의 단어들에 대한 정보를 인덕스로 바꿔주어서 우리 모델의 입력으로 넣을 수 있도록 하고요. 이어서 마스크를 만든 뒤에 실제로 인코더에 이러한 소스 문장을 넣어서 출력 값을 구할 수 있도록 합니다. 그다음에 이제 실제 출력 문장은 SOS 토크부터 출발해서 맥스 랭스까지 한 번씩 반복적으로 모델의 디코더에 넣어서 출력 값을 만들어 낼 수 있도록 합니다. 그래서 이때 매번 디코더에 넣었을 때 가장 마지막 단어가 출력 문장으로써 하나씩 추가가 되는 거고요. 그래서 결과적으로 최대 맥스 랭스만큼 반복을 하다가 EOS를 만난의 순간에 거기에서 멈추도록 해서 그때끈이 출력된 모든 단어들이 전체 출력 문장이 되겠습니다. 그래서 실제 결과로 뽑기 위해서 각각의 인덕스를 다시 문자열로 바꿔서 리턴해주는 것을 알 수 있습니다. 그래서 간단하게 한 번 테스트 데이터 중에서 열 번째 데이터를 확인해 보도록 할게요. 자, 대용 확인해 보시면 얘가 독일어 문장이 되겠습니다. 이 내용이 원래 의미하는 내용은 한 명의 어머니와 그의 자식은 노래를 부른다 야외에서 좋은 나를 즐기며 라는 내용이죠. 이제 이게 실조로 출력된 결과를 하긴 해 보면 우리 모델이 번역한 내용은 어머니와 그의 어린 나들이 야외에서 작은 나나를 즐기고 있다. 이런 식으로 번역되어 있는 걸 확인할 수 있습니다. 완전히 동일한 문장을 아니더라도 이런 전반적인 의미 자체가 잘 전달되도록 번역이 이루어진 걸 확인할 수 있습니다. 또한 이렇게 출력된 어색션 값은 총 8개의 해드로 구성된 어색션 스코어들의 지팝이라고 할 수 있는데요. 전체 그림에다가 각각의 헤드에 대한 어색션 스코어 값을 출력하도록 만들 수 있습니다. 자 그래서 이와 같이 실제 그림까지 출력하도록 만들어보시면 자 이렇게 각각의 단어가 출력되기 위해서 소스 문장에서의 어떤 정보를 많이 참고 있는지를 알 수 있습니다. 이 영어 문장으로 마더와 이 독일어 문장의 해당 단어는 마찬가지로 어머니라는 의미를 가지고 있습니다. 그렇기 때문에 이렇게 마더라는 단어를 출력하기 위해서 독일어 문장이 이러한 단어를 참고했다는 것을 시각적으로 확인할 수 있습니다. 이와 같이 총 8개의 헤드 가까이 돼서 어색션 스코어 값이 며여지는 걸 확인할 수 있고요. 말씀드렸듯이 가까이 헤드들은 서로 다른 어색션 컨셉들을 학습하도록 만들어지기 때문에 이렇게 각각 어색션 스코어 값이 구해진 가구가 조금씩 다를 수 있다는 점 또한 확인할 수 있습니다. 이제 마지막으로 간단하게 블루 스코어를 계산해서 학습이 완료된 트레스폼원 모델에 스코어를 구해 볼 수 있습니다. 지금 예시에서는 가까이 입력 문장이 있을 때 그 정담 문장 또한 하나씩만 존재하기 때문에 하나의 예측 값에 대해서 그 정답 값이 1대 1로 매칭될 수 있도록 바로 그 정담 문장을 하나의 리스트로 감싸서 하나씩 나열하는 걸 알 수 있습니다. 자 그래서 이와 같이 문장 100개당 한 번씩 예측과 정답 값을 출력하도록 만들어 본 거고요. 자 이렇게 실조로 문장을 보시면 꽤 유사하게 예측이 이루어진 걸 확인할 수 있습니다. 예를 들어 마지막 문장 같은 경우는 나이가 많은 한 남자는 미디오 게임을 플레이하고 있다라고 번역을 했는데요. 실제 정답 값과 비교했을 때 거의 유사하게 번역을 생한 걸 확인할 수 있습니다. 또한 여기 보이는 이 블루 4 스코어가 일반적으로 알려져 있는 블루 스코와 같은 값을 가진는데요. 이렇게 블루 스코 값으로는 36을 얻은 것을 확인할 수 있습니다. 이어서 같이 한 번 노문 리딩 진행해 보도록 하겠습니다. 노문의 제목은 얻텐션 이슬 올려니드이고요. 말 그대로 얻텐션만 잘 활용해도 높은 성능을 얻을 수 이따 라는 의미를 가지고 있습니다. 본 노문은 2017년도 립스에 구글팀이 발표한 노문이고요. 말씀드렸듯이 이전까지에는 어렰는과 얻텐션 메커니즘을 같이 활용하는 기법들이 많이 사용이 되었었는데요. 본 노문에서는 어렰는 미디시에는 전부 사용하지 않고 오직 얻텐션 기법만 사용해서 기계원역 테스트에서 좋은 성능을 얻었습니다. 바로 한 번 앱스테이크부터 읽어볼게요. 보시는 바와 같이 이전까지는 인코드와 디코더를 포함한 형태로 복잡한 어렰는 혹은 시에는 기반에 시콘스 간 변형이 이루어지는 어렰는 미디시에는 기반으로 하는 모델을 많이 사용했습니다. 여기에서 트레스 덕션은 변호와 혹은 형질의 변형과 같은 의미를 가지고 있는데요. 말 그대로 어떤 시콘스 간 변형을 의미하는 겁니다. 대표적인 예시가 기계 번역이 있겠죠. 이제 그런 테스트에서 이렇게 어렰는 혹은 시에는 전적으로 활용한 모델이 많이 사용되었습니다. 또한 그런 인코드 디코드 아키텍처의 어땠션 메커니즘을 활용했을 때 보다 성능이 좋아질 수 있었다는 그런 결과를 이전 노문에서 확인해 볼 수가 있었죠. 그래서 본 노문에서는 이렇게 트레스 포멀한 이름에 아키텍처를 제안하고 이 아키텍처는 전적으로 어땠션 메커니즘에 기반을 하고 있는 아키텍처입니다. 이때 말 그대로 리커런스 나 컨보루션 자체를 제거한 형태로 어땠션 메커니즘만 활용했다는 거고요. 어땠션 메커니즘 활용함으로써 리커런스에게 각각의 시콘스를 처리할 필요가 없어지기 때문에 그냥 행렬곡을 이용해서 완전히 병렬적으로 시콘스테이터를 처리할 수 있기 때문에 훨씬 더 빠르게 처리가 가능하다는 점이 장점입니다. 그래서 결과적으로 굉장히 유명한 두 가지 테스크인 WMT14용도 데이터셋을 이용해서 영어를 독일어로 번역하는 테스크 그리고 영어를 불어로 번역하는 테스크에서 각각 훨씬 개선된 성능을 보여준 것을 확인할 수 있습니다. 그래서 이와 같이 두 가지 테스크에 대해석 완전히 스텔 도우드의 알트의 성능을 보인 것을 확인할 수 있습니다. 또한 이와 같이 여덟개의 피백 GPU를 이용해서 상대적으로 더 적은 시간을 들여서 학습을 마치일 수 있었다고 하고요. 이는 이전까지 제한되었던 모델가 비교했을 때 훨씬 더 학습 효율이 높다고 할 수 있습니다. 이러한 트레스 풍원은 비단 기계 번역 뿐만 아니라 시콘 스테이터를 처리하는 다양한 테스크에 대해서 일반화가 가능하고 성능이 잘 나오는 것도 또한 보여주었습니다. 대표적으로 구문 분석 분야에서의 이러한 컨스티치현식 펄싱 테스크에 대해서도 잘 동작하는 걸 확인할 수 있었다고 합니다. 이전에 이와 같이 어르는 그리고 LSTM 그리고 지하려와 같은 다양한 모델들이 제안이 되었고요. 이러한 네토어 그들은 시콘스 모델링을 위해서 효과적으로 사용이 되고 있었습니다. 다만 기본적으로 이러한 리커롱트 모델들은 한 번에 한 단어싱 넣는 방식처럼 시콘스에 포함되어 있는 각각의 토큰들에 대한 순서 정보를 먼저 정렬시킨 뒤에 이것을 반복적으로 입력으로 넣어서 이러한 희든스테이트 값을 갱신시키는 방법으로 동작을 합니다. 그렇기 때문에 이런 식으로 리커롱트하게 동작하는 모델링 경우 시콘스의 길이 즉 토큰의 개수만큼 릴렉터 그의 입력을 넣어야 되기 때문에 당연히 병려적인 처리가 어렵다는 문제가 존재하고요. 다시 말해 레이어의 아웃포슬 핸려부로 바로 구할 수 있는 게 아니라 즉, 번역에서는 문장의 길이 만큼 입력을 수행할 필요가 있기 때문에 이는 메모리 밑속도 침면에서 비율성을 약이할 수 있다고 지적하고 있는 것입니다. 이어서 엇텐션 메커니즘이 등장을 했었는데요. 엇텐션 메커니즘을 활용하면서 매번 출력 단어를 만들어낼 때마다 소스 문장에 출력 정보 중에서 어떤 정보가 가장 중요한지에 대해서 가중치를 부여하도록 해서 그런 가중치가 적용되어 고패진 히든스테이트 값을 이용하도록 해서 출력 단어를 보다 효과적으로 생성할 수 있도록 만들 수 있다고 했죠. 다만 이런 엇텐션 메커니즘도 기본적으로 어레는 것과 같이 사용되는 경우가 많았고요. 그래서 본 논문에서는 그냥 리커런스 특성 자체를 완전하게 제거해 본 겁니다. 즉, 다시 너래에서 완전히 엇텐션 메커니즘에 전적으로 의존해서 모델의 결과를 내보낼 수 있도록 합니다. 엇텐션 메커니즘을 활용하기 때문에 한 번의 행열국으로 위치정보가 포함된 전체 시컨스를 한 번에 처리할 수 있다는 점에서 다시 말해 순차적으로 입력을 넣지 않아도 되기 때문에 병력 처리가 가능하다고 볼 수 있는 것입니다. 그래서 이런 특징을 활용했던이 성능이 훨씬 좋아진 걸 확인할 수가 있고 8개의 P100 GPU를 이용해서 학습을 해본 결과 현실적인 시간 즉 12시간 만에 상당히 좋은 베이스 모델의 성능을 얻을 수 있었다고 말하고 있습니다. 그래서 기반이 되고 있는 다양한 백그라운드 논문을 소개하고 있고요. 이 중에서 셀프 엇텐션이라고 하는 것은 타겐 문장을 만들기 위해서 소스 문장에서의 히든 정보를 참고하는 것이 아니라 어떠한 문장이 있을 때 자기 자신의 문장 스스로에게 얻텐션을 수행해서 레프레젠테이션을 열링할 수 있도록 만들어주는 게 셀프 엇텐션 메커니즘입니다. 다시 말해 하나의 시컨스가 있을 때 그 시컨스에 포함되어 있는 서로 다른 위치에 대한 정보가 서로가 서로에게 가정치를 부여하도록 만들어서 하나의 시컨스에 대한 레프레젠테이션을 효과적으로 학습하고 표현할 수 있도록 만들어주는 것입니다. 예를 들어 아이, 엠머, 티처라는 하나의 문장이 있을 때 내게 단어들은 서로가 서로에게 오탠션을 수행해서 가정치를 부여하도록 할 수 있다는 거죠. 자, 그래서 본 논문에서는 이와 같이 트랜스 부모는 전적으로 이러한 오탠션 메커니즘에 기반하는 사실상 최초에 시컨스가 변형이 가능하도록 만들어준 네트워크 라고 할 수 있고 본 논문은 이런 아이디어를 통해서 매우 좋은 성능을 이끌어낼 수가 있었고요. 그래서 결과적으로 비교적 최근에 나온 GPT나 벌드와 같은 다양한 아키텍처들은 이러한 트랜스 부모에서 제한되었던 아키텍처를 많이 따르고 있습니다. 자, 그래서 시컨스가 변형 모델에 대해서 많은 아키텍처는 인코더 디코더 구조를 따르고 있고요. 여기에서 35번 논문 같은 경우는 원본 시컨스트 시컨스트 논문을 의미하고요. 이런 식으로 X1부터 XN까지 총 NG에 토크 누룽 구성된 입력 시컨스가 있을 때 이것을 컨텐ュğ스한 어떤 인베딩 맥터로서 바꾸어 주고 이런 인베딩 맥터인 G가 들어왔을 때 이 디코더는 Y1부터 YM까지 총 NG에 토크 누룽 구성된 출력 문장을 만드는 방식으로 동작합니다. 이때 기본적으로 어렸넨 구조를 따르고 있는 모델들은 오토리 그레 시그하게 시컨스의 길이 만큼 옅토크 입력에 주어진 방식으로 동작합니다. 다시 날에 이전 단계에서 생성되었던 신보를 이용해서 다음 번에 나올 출력 값을 만드는 방식으로 동작한다는 거죠. 엄밀이 말하면 트레스 풍원도와 마찬가지로 인코더와 디코더 파트로 구성이 되어있으며 이러한 기본적인 아키텍처는 활용하고 있습니다. 단지 다른 점이라고 한다면 모델을 리커런트 하게 이용하지 않고 옅텐션 맥커드지만 활용해서 시컨스에 대한 정보를 한 번에 입력으로 준다는 점이 그 특징이라고 할 수 있습니다. 자, 그래서 여기 보이는 그림이 전체 트레스 풍원도에 아키텍처 라고 할 수 있는데요. 자, 이와 같이 어렸넨을 사용하지 않는 대신에 문장내에 포함되어 있는 각각의 단어들의 위치 정보를 인코딩해서 입력하기 위해 포지션을 인코딩을 사용하고요. 이렇게 입력 임베딩과 같은 디메전으로 합치기를 쓰응해서 이렇게 만들어진 결과를 실제 임베딩 맥터로써 사용을 하고요. 이제 애가 이렇게 코리키 멜류 값으로 각각 복제가 되어 입력됩니다. 여기 보이는 멀티 헤드 엇텐션은 셀프 엇텐션으로 동작을 하며 보시는 바와 같이 코리와 키아 멜류 값이 모두 동일합니다. 또한 엇텐션은 입력과 출력이 차원이 갖다고 말씀드렸죠. 그래서 결과적으로 이렇게 들어갈 때의 차원과 이 엇텐션은 나왔을 때의 차원은 동일하구요. 마찬가지로 레지도로 커리션을 세형한 뒤에 정규화를 세형해 주고 이렇게 feed4를 레이어를 거치고 다시 한 번 정규화를 세형할 때까지 전부 다 각각의 레이어에 대한 입력과 출력이 디멘전은 갖다고 보시면 됩니다. 그래서 이제 이러한 과정 자체를 총 앤번만큼 반복해서 총 엔계에 인코덜 레이어가 차곡차곡, 반복적으로 수행이 돼서 마지막에 나온 그 출력 값을 이와 같이 매인코더 디코더 엇텐션에서 사용할 수 있도록 만든다고 보시면 되겠습니다. 또한 이렇게 디코더 파티에서는 지금까지 출력된 단호만 얻텐션할 수 있도록 하기 위해서 학습을 수행할 때 이렇게 마스크를 씌워서 뒤쪽에 있는 단호는 미리 알지 못하도록 막는 방식으로 모델이 정상적인 데이터만을 학습할 수 있도록 만들어주고요. 마찬가지로 여기 보이는 이 디코더의 첫 번째 엇텐션에서는 코리아키 웨리올 값이 갔기 때문에 셀프 엇텐션이 생된다고 할 수 있습니다. 이렇게 두 번째 엇텐션에서는 이 코리아 값이 디코더에 있기 때문에 각각의 출력 단호를 만들기 위해서 이 인코더 파티에서 어떠한 정보를 참고하면 좋은지를 엇텐션을 수행한다고 할 수 있는 겁니다. 즉, 다시 말해 이 키와 웨리올 값들은 인코더 파티에서 봤습니다. 그래서 마찬가지로 피드프워드 레이어를 거친 뒤에 마지막에 리�니얼 레이어로 거치고 소프트 맥스를 취해서 실제로 각각의 출력 문장에 포함된 단호들이 실제로 어떤 단호에 해당하는지 구할 수 있도록 할 수 있습니다. 또 здесь 여기에서 추가적으로 레이블 스무씨을 적용해서 정규와 효과를 더해서 성능을 더 높일 수 있습니다. 그래서 이러한 트레스 폼오의 아키텍션은 향후 많은 논문에 영향을 미치게 되었습니다. 그래서 선호했던 내용과 동일하게 인코더 파티는 기본적으로 여러분 인코더 레이어가 중처비되어 사용이 되고요. 보시는 바와 같이 본 논문에서는 총 6번 인코더 레이어를 중처내서 사용할 수 있다고 말했고요. 이와 같이 레질들 커리션을 이용해서 노멜라이저션을 거치기 전에 그 입력 값으로 아이덴트팀의 빙위를 쎈갈 수 있도록 만들어주었고요. 또한 이와 같이 인베딩 데이터에 차오는 512 차원으로 설정했다고 말하고 있습니다. 마찬가지로 디코더에서도 총 6개의 디코더 레이어를 싸울 수 있도록 만들었고 실질 구현상에서 이와 같이 인코더와 같은 레이어에 개수를 가지고 가지고 만들인 경우가 많습니다. 그래서 멀티도 얻텐션을 쎈할 때 인코더에 아웃급 값에 대해서 얻텐션을 쎈갈 수 있도록 만든다고 했고요. 마찬가지로 동일하게 커리션을 사용해서 학습 날이돌을 낮추어서 보다 좋은 글러블로 업티마를 찾을 수 있도록 모델을 설계했습니다. 또한 디코더 파트에서 쓰이는 셀프 엇텐션에서는 이전에 등장한 단원들만 참고할 수 있는 형태로 마스크를 씌워서 마스크가 부튼 형태에 멀티도 얻텐션을 사용할 수 있도록 만들었다고 합니다. 자, 그래서 여기에서는 엇텐션 메크노지메 대에 설명하고 있는데요. 이때 하나의 코리는 말 그대로 어떠한 질문을 날린 겁니다. 특정 키에게 물어보는 것과 같습니다. 제기때 코리라고 하는 것은 말씀드렸듯이 질문을 하는 주차라고 할 수 있고요. 이때 이 키는 엇텐션을 쓰이는 대상이라고 할 수 있습니다. 예를 들어서 사랑해라는 하나의 단어가 생성되기 위해서 iLoveU라는 문장에 포함된 단어들 중에 어떤 단어가 가장 중요했는지를 물어보는 방식으로 각각의 코리가 키에 대해서 엇텐션을 쓰이는 메커는 집으로 이해할 수 있습니다. 그래서 여기 보이는 그림이 실제 멀티도 얻텐션을 잘 사용하고 있는 그림이고요. 이러한 멀티도 얻텐션은 각각의 인코더와 디코더 레이어에서 사용된다고 했습니다. 이때 멀티도 얻텐션은 내부적으로 스케일리닷 프로더 버템션을 가지고 있는데요. 얘는 바로 왼쪽에 있는 그림과 같이 생겼습니다. 자 이때 이렇게 코리아 키아 멜류가 들어게 되면 각각의 코리가 이 키에 대해서 질문을 하는 내용이 바로 이렇게 행료몰으로 이루어지고요. 또한 소프트 맥스에 들어가는 값에 대해서 스케일링을 하기 위해 스케일 레이어를 포함합니다. 이때 스케일 레이언은 여기 들어오는 키의 차원에 루트를 시원 값을 나눠줄 수 있는 형태로 사용합니다. 또한 이렇게 마스크 맥터 같은 경우는 필요할 때 사용을 하고요. 그래서 유화같이 소프트 맥스를 취해서 각각의 키에 대해서 얼마나 중요한지에 대한 값을 확률 형태로 표현할 수 있도록 한 뒤에 이제 그런 확률 값을 각각 실제 멜류와 곱해서 오테션 멜류 값을 만들어 낼 수 있는 겁니다. 이제 이러한 가정들이 각각의 헤드 마다 서로 다르게 이루어진 뒤에 다시 이렇게 결과를 합쳐서 리�니얼 레이어를 거친 뒤에 아웃플 값을 내보는다고 할 수 있습니다. 자 이때 실제 구현상으로는 이렇게 입력 값이 들어왔을 때 입력 값은 이 V와 K와 키에 대해서 각각 복제가 되어서 들어가도록 할 수 있고 이때 각각의 리�니얼 레이어에 인배딩 차원을 키, 커리, 웨르에 차원으로 바꿔줍니다. 이제 그렇게 나온 값들을 각각 어떤 선을 수행한 뒤에 동현한 차원이 나오게 되면 다시 인해들을 묶어 주어서 기존의 인배딩 차원과 결과적으로 같은 차원에 될 수 있도록 만들어준다는 거죠. 또한 여기에서 실제 구현할 때는 예를 들어서 이렇게 입력으로 들어오는 인배딩 차원이 521라고 하고 이 H가 8이라고 한다면 각각의 리�니얼 레이어에는 64차원으로 맵핑을 해주는 것입니다. 다만 여기에서 실제 구현할 때는 그냥 517, 511로 병렬적으로 그냥 한 번의 행렬구별 구한 뒤에 그 결과 값을 8대로 쫄게 해서 사용할 수도 있습니다. 자, 그래서 실제로 오테션 메커니즘에서 핵심이 되는 스케일리다트 프라더 오태션에 대해서 설명하고 있는데요. 자, 확인해 보시면 이렇게 실제로는 행렬 형태로 한 번의 코리와 키들을 묶어서 병렬적으로 계산할 수가 있고요. 말씀드린 내용과 마찬가지로 먼저 코리랑 키란 곱하구요. 이때의 코리와 키는 기본적으로 차원이 갓도록 만들어서 이와 같이 곱쌤이 생댈 수 있도록 만듭니다. 이제 그래서 스케일리다트 만큼 나눠준 뒤에 헝유값을 구하고 얘를 실제로 이 벨력값과 행렬구별 사용할 수 있다고 했죠. 자, 그래서 이러한 방식은 에리티브 오템션과 약간 구결되는 방식인데요. 바로 여기 확인해 보시면 이렇게 키란 코리를 그냥 곱해 가지고 한 번의 오템션을 구할 수가 있습니다. 사실 우리가 앞서 확인했던 시코스트 시코스트 오템션 메커니즘을 활용했어야 되는 이 코리와 키가 특정한 행렬곱에 함께 입력되는 형태로 동작을 했었는데요. 여기서는 그냥 코리아 키를 바로 서로 곱하도록 만들어서 Not Pruduh Attention 형태로 사용했다고 볼 수 있고요. 노무현에서 이와 같이 Not Pruduh Attention을 사용했을 때 다른 순위 내조건 이용해서 얻은 생각을 수행하는 것이 프레cticlea 빠르고 공간 효율적이었다고 말하고 있습니다. 또한 이렇게 내조건 이용하는 방식인 경우 스케일링을 하지 않으면 결과가 많이 안 좋았다고 하는데요. 본 노무현에서는 그러한 이유를 다음과 같이 추측하고 있습니다. 바로 소프트 넥스 같은 경우는 이 중간 부분이 그레디언트가 상대적으로 크고 사이드로 가면 갈수록 그레디언트가 작아지는 특징을 가지고 있는데요. 그렇기 때문에 값이 너무 커지거나 하면 너무 그레디언트가 작아질 수 있어서 학습이 잘 안 될 수가 있겠죠. 그렇기 때문에 특정 스케일 팩터 만큼 고패 주워서 값을 작게 만들어 학습이 잘 잃어질 수 있도록 만드는 것입니다. 또한 말씀드렸듯이 기존의 인보딩 차원을 딘 모델이라고 했을 때 얘를 H만큼 즉 헤드의 개수만큼 나누어서 각각 키나 웨류 코리아 같은 차원들이 결정할 수 있다고 했고요. 이러한 백터들은 나중에 다시 이어 부쳐지기 때문에 별가적으로는 입력과 출력에 dimen 전이 각도록 만들 수 있습니다. 자, 그래서 멀케이드 얻은 저는 말 그대로 헤드가 여러개라는 의미라서 이렇게 멀케이드 라고 이름이 붙은 거고요. 이와 같이 각각의 헤드에 대해서 전부 다 얻은 저는 쓰행한 값을 다시 이렇게 이어 부친 뒤에 아웃컵 값을 내보내기 위해서 행료급을 생갑니다. 이제 여기에서 이 아이 값은 각각의 헤드에 대한 인덕스라고 할 수 있고요. 그래서 이와 같이 실제로 코리나 키를 만들기 위한 행료를 크기는 D 모델 곱하기 DK가 사용이 되고요. 다시 말해서 D 모델 차원에 인베딩 맥터를 DK 차원에 코리아 키 맥터의 차원으로 만들 수 있도록 하는 것입니다. 물론 말씀드렸듯이 실제로 구려날 때는 그냥 D 모델 곱하기 D 모델만큼의 행료를 곱한 뒤에 그냥 결과값 자체를 나누어서 사용할 수도 있는 거예요. 그래서 본 논문에서는 D 모델을 512 그리고 AG를 팔로 설정해서 코리 맥터의 차원을 64라고 설정을 했고요. 또한 언니를 말하면 이 멜료값 같은 경우는 차원을 코리 혹은 키와 똑같이 맞출 필요는 없지만 여기서 이렇게 코리키 멜료 전부 다 같은 차원은 64차원으로 사용할 수 있다고 말하고 있습니다. 그래서 이제 이러한어 텐션이 실제로 어디에 쓰이는지 확인해 보시면 앞서 말씀드렸듯이 총 3가지 위치에서 사용이 되는데요. 모든어 텐션은 기본적으로 다 헤드가 여러 개인 멀티 헤드어 텐션이며 사용되는 위치에 따라서 3가지로 구분 되는 것입니다. 먼저 인코르 디코드어 텐션은 디코드 파트에서 사용이 되는 거고요. 이때 이 코리는 이 디코드에서 오는 것이고 이때 이 키와 멜료값은 인코드의 출력 파트에서 가져온다고 했어요. 이 내용은 다시 말하면 우리가 출력 단어를 만들기 위해서 소스 문장에 포함되어 있는 단어들 중에서 어떤 정부에 보다 초점을 맞춘한 되는지를 계산하는 과전이라고 비교할 수 있다고 했죠. 이어서 셀프어 텐션은 기본적으로 코리와 키와 멜료가 모두 같은 형태를 의미하고요. 바로 인코드 파트에서 그대로 사용이 됩니다. 그리고 디코드 파트 또한 마찬가지로 맨 처음에 입력 임베딩이 들어왔을 때 셀프어 텐션을 사용할 수 있다고 했는데요. 여기서는 마스크를 씌워서 다시 말해 소프트 맥스에 들어가는 값이 마이너스 무한이 될 수 있도록 해서 0%가 부여될 수 있도록 하여 각각의 단어가 앞부분에 있는 단어에 대한 정보만 참고할 수 있도록 만들었다고 보시면 되겠습니다. 또한 이와 같이 포지션 와이즈 휴드4 레톡을 사용할 수 있다고 했는데요. 여기 보이시는 이 맥스폰 시원 터문 렐루 액탭에 대한 내용을 보여주고 있는 것입니다. 그래서 이와 같이 입력 값과 출력 값은 모두 같은 차원을 가지게 되고요. 이렇게 중간에 히든 디메전으로 약간 고차원 공간에 맵핑이 되었다가 술령 레이어를 통해서 휴드4 월드가 생댈 수 있도록 만든다고 보시면 되겠습니다. 또한 마찬가지로 임베딩과 소프트 맥스가 사용이 되었는데요. 이건 이제 기본적으로 시컬스트 시컬스 모델에서 사용되는 내용과 갖다고 할 수 있습니다. 그냥 인프 디메전 즉 특정한 언어의 포함되어 있는 단어에 개스와 비례하는 그런 인프 디메전이 들어왔을 때 임베딩 레이어로 거쳐서 임베딩 차원 즉 디모델 차원으로 맵핑해주는 겁니다. 또한 이어서 트랜셋 보머에서는 오탠셋 메컨이지만 전적으로 활용하기 때문에 위치에 대한 정목값을 같이 주기 위해서 인코딩 정보를 같이 더해서 넣어줄 수 있다고 했습니다. 다시 날에 리커러스 및 컴퍼로션 둘 다 사용하지 않기 때문에 위치에 대한 정보를 같이 넣어줄 필요가 있는 겁니다. 그래서 보머 논문에서는 다음과 같이 사인과 코사인과 같은 주기암수를 사용해서 인력은 넣었다고 하고요. 말씀 드렸듯이 우리가 위치에 대한 정보를 넣어줄 때 꼭 사인이나 코사인 암수를 이런 형태로 사용할 필요는 없고요. 이런 함수를 사용하지 않고 이런 임베딩 레이어동안 우리가 별도로 학습하도록 만들어서 네토크를 구성할 수도 있습니다. 실제로 본 논문에서는 이렇게 학습이 가능한 형태로 인베딩 레이어로 사용해봤다고 하는데요. 실제로 성능상에는 차이가 없었다고 합니다. 사실 그렇기 때문에 우리가 파이톨치와 같은 프레이멈을 사용할 때 그냥 학습하도록 하는 게 구현이 더 쉬울 수도 있기 때문에 실제로 우리가 아까 실습을 진행했을 때 별도에 학습 가능한 인베딩 레이어를 사용을 했던 겁니다. 다만 본 논문에서는 이러한 정연파험수를 사용했을 때 보다 긴 시콘스가 들어왔을 때 성능이 더 잘 나올 수 있다고 언급하고 있습니다. 이어서 섹션 4에서는 이러한 슬프어 텐션이 왜 더 유리한 가에 대해서 설명하고 있는데요. 본 논문의 조자들은 세 가지의 어떠한 열망하는 그런 장점들 목표로 두고 이러한 슬프어 텐션을 고환했다고 말하고 있는데요. 첫 번째로는 각각의 레이어마다 계산 복잡도가 주로 든다는 장점이 있습니다. 또한 리컬언스를 없애물어서 병렬적인 처리가 가능하고요. 마지막으로 롱 레인지 디페드시에 대해서도 잘 처리할 수 있다고 말하고 있습니다. 자, 그래서 이렇게 위쪽에 보이는 테이블이 어란에는 사용했을 때와 컴프로션을 사용했을 때 그리고 이렇게 오탄션 메커니즘을 활용했을 때에 대한 효율성을 분석하고 있는 건데요. 이때에는 시콘스의 길이 즉 단어의 개소라고 할 수 있고요. 이때 확인해 보시면 이런 오탄션 기법을 사용할 때 시코셔 데이터를 처리할 때 단 한 번에 병렬적으로 구할 수 있기 때문에 아래는가 비교했을 때 훨씬 네트워크에 들어가는 인력의 횟수가 적다는 걸 확인할 수 있고요. 이와 같이 복잡도를 비교했을 때에도 이 애는 단어의 개수기 때문에 일반적으로 이 디보다는 조금 더 작게 형성되는 경우가 많습니다. N 재곡곡 파기 D가 N곡 파기 D 재곡보다는 더 낮을 확률이 높기 때문에 보다 유리한 복잡도를 가진다고 할 수 있습니다. 실제로 이렇게 본문에서도 내용이 설명되고 있는데요. 보통 이 N 즉시코인스의 길이가 이 디보다는 짧은 경우가 많기 때문에 훨씬 효율적일 수 있다고 말하고 있습니다. 실제로 다양한 번역 모델에서는 이 N 즉시코인스 길이는 이렇게 단어 단위의 토크 내 개수와 갖다고 할 수 있죠. 그런 경우에 사악해 보았을 때 확실히 이 N의 일반적으로 디보다는 작게 형성되는 경우가 많습니다. 실제로 다양한 번역 데이터에서 확인해 보시면 한 문장에 포함되어 있는 단어의 개수가 그렇게 많지 않기 때문에 그런 측면이 보았을 때 단어의 개수가 이러한 N이 되기 때문에 그런 측면에서 효과적이라고 할 수 있습니다. 또한 추가적으로 엇텐션 메콘이집 자체가 우리 뉴럴 퇴크를 보다 3명 가능한 형태로 만들어준다는 점이 장점이라고 할 수 있습니다. 실제로 우리는 각 단어를 출력할 때 소스 문장에서 어떤 단어를 가장 많이 참고해서 만들었는지 시각적으로 출력해 볼 수 있습니다. 단순히 그냥 각각에 헤드에 포함되어 있는 그 셀프어 텐션 메콘이집에 소프티넥스 값을 출력해 보면 되겠죠. 또한 트레이닝을 진행할 때 설정했던 내용들은 다음과 같은데요. 말씀드렸듯이 영어 독이로 그리고 영어 프랑스어 이 두 가지 대폐적인 데이터 셋지에 대해서 실험했다고 하고요. WM-D 2014년도 영어 독어 데이터 셋은 약 450만 개 정도에 문장 쌍이 존재하고요. 그리고 영어 프랑스어 같은 경우는 3600만 개 정도에 문장 쌍이 존재하는 데이터 셋을 이용했고요. 학습을 위한 하드웨어로는 8개의 NVIDIA P-100 GPU를 사용했고요. 베이스 모델 마늘도 스테이도 VDRT의 성능이 나오고 학습시간 또한 내고 빠른 12시간 밖에 걸리지 않았다고 말하고 있습니다. 또한 이와 같이 에덤 업팀 아이점을 사용하고 세부적인 파람이터는 단어가 같습니다. 또한 정규어 효과를 위해서 레지지어 럴링을 사용할 때 이 드라바웃도 같이 사용할 수 있도록 만들었고요. 이어서 레이블 값을 넣어줄 때 이 스무신 기법까지 정용하도록 만들어서 우리 모델이 특정출력 값에 대해서 확실을 가지지 않도록 함으로써 정규어 효과를 더할 수 있다고 했습니다. 사실 이것도 굉장히 잘 알려진 정규어 기법이고 이미지 분류 등에서도 이미 많이 사용되고 있는 기법 중 하나죠. 그래서 이와 같이 어큐러쉬와 블루스코어를 높일 수 있었다고 말하고 있습니다. 그래서 여기 보이는 표가 실제 트레스폰와 아키텍처에 성능을 잘 보여주고 있는데요. 이와 같이 기본적인 베이스 모델만 가지고도 기존 스테이도 VDRT-NET와 필적하는 좋은 성능을 내는 걸 확인할 수 있었고요. 이때 학습시간은 훨씬 잡았다는 걸 확인할 수 있습니다. 또한 이러한 트레스폰 모의 파라미터스를 훨씬 늘려서 큰 모델을 사용했을 때에도 이전 연구와 비교했을 때 학습 효율이 높았으며 성능은 훨씬 더 개선된 걸 확인할 수 있습니다. 또한 이런 트레스폰와 아키텍처에서 어떤 컴포넛을 들어 상대적으로 중요한지에 대한 내용을 확인하기 위해서 모델 베리에이션 실험 또한 진행을 했는데요. 간단하게 헤드위수를 주려보거나 특정 파라미터위스로 늘려보거나 주려보거나 이런 실험들을 해본 겁니다. 첫 번째로 헤드위수를 바꿔 가면서 실험을 해본 건데요. 이렇게 여기 보이는 이 베이스 모델이 가장 기본적으로 사용한 하이파 파라미터라고 할 수 있고요. 여기서 L 같은 경우는 이 헤드위에 디멘전을 바꿔가지고 그에 따라서 이 키와 뇌류의 디멘전 또한 바뀔 수 있도록 한 겁니다. 말씀드렸듯이 D 모델을 H로 나는 값이 키와 뇌류의 디멘전으로 사용될 수 있다고 했죠. 그래서 확인해 보시면 이렇게 헤드를 8개 사용했을 때 가장 성능의 좋은 걸 확인할 수 있고요. 또한 비 같은 경우는 별도로 헤드와 상관없이 이 키, 뇌류의 디멘전을 더 주려본 겁니다. 확인 결과 당연히 파라미터일 수가 줄어서 모델의 캐퍼시티 또한 감소하게 되겠죠. 실조를 결과 또한 더 안주아지는 걸 확인할 수가 있었고요. 그래서 이렇게 64회 사용했을 때가 16이나 31회상했을 때보다 더 좋은 걸 확인할 수 있습니다. 또한 이렇게 모델의 크기를 더 키웠을 때 더 성능이 좋아진 것도 한 확인할 수 있습니다. 보시면 이런 식으로 인베등 처음을 높이거나 이 피드4를 레이어에 포함되어 있는 처음을 높이었을 때 더 성능이 좋아지는 걸 확인할 수 있고요. 또한 드라바웃 기법은 이처럼 오벌필팅 방지에 매우 효과적인 걸 확인할 수 있습니다. 그래서 이렇게 드라바웃에 썼을 때 더 성능이 많이 좋아진 걸 확인할 수 있습니다. 또한 위치에 대한 정보를 주기 위해 사인가 코사에 남수로 이용한 인코딩 대신에 별도에 인베등 레이어를 사용했을 때 이때는 베이스 모델과 비교했을 때 성능의 차이는 거의 없는 걸 확인할 수 있습니다. 또한 이러한 트래스 부모는 비단 기계 보내 뿐만 아니라 다양한 자연어 처리 테스트에서 사용이 가능한데요. 대표적으로 구문 분석 분야에 대해서 실험한 결과 또한 보여주고 있습니다. 여기 테이블 4번이 그 내용을 간단하게 보여주고 있고요. 여기 확인해 보시면 이 경우를 제외하고 나머지는 다른 스텔 노브대 알트 네트워크가 비교했을 때 더 성능의 좋은 걸 확인할 수 있습니다. 세미스포 바이스드 케이스에 대해서도 마찬가지로 더 좋은 성능이 나오는 걸 확인할 수 있습니다. 그래서 결과적으로 이와 같이 본농문에서는 트랜스포 마케팅처를 제안했고요. 본농문은 기존까지 노문과 다르게 전적으로 엿텐션 메커니진만 활용을 해서 리크론트한 네트워크 자체를 전부다 아켓택처에서 빼버렸고요. 이로윈에 보단 높은 병렬성을 얻게 되고 성능 또한 많이 개선될 수 있었습니다. 그래서 실제로 기계 번역 테스트에 대해서 기존까지 존재했던 다른 아켓택처에 비해서 더 좋은 성능을 보여줄 수가 있었고 비단 기계 번역 뿐만 아니라 다양한 테스트에 대해서도 적용 가능성이 높다는 것까지 잘 보여주었습니다. 이상으로 이번 시간에는 트랜스포 마케팅에서 알아보았습니다.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")  # tiny, base, small, medium, large 중 선택 가능\n",
    "result = model.transcribe(\"[딥러닝 기계 번역] Transformer： Attention Is All You Need (꼼꼼한 딥러닝 논문 리뷰와 코드 실습).mp3\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoT5\n",
    "https://github.com/wisenut-research/KoT5/blob/master/kot5_hf/README.md\n",
    "\n",
    "https://huggingface.co/wisenut-nlp-team/KoT5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jimmy/anaconda3/envs/SST/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer = AutoTokenizer.from_pretrained(\"wisenut-nlp-team/KoT5-small\")\\n\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"wisenut-nlp-team/KoT5-small\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wisenut-nlp-team/KoT5-small\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"wisenut-nlp-team/KoT5-small\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\n\\ntext = \"ASR Recognized Text:\" + result[\"text\"]\\ninputs = tokenizer(text, return_tensors=\"pt\").to(\\'cuda\\')\\n\\n# 디코딩 없이 모델 추론 수행\\nwith torch.no_grad():\\n    output_ids = model.generate(**inputs, max_length=128)\\n\\n# 생성된 토큰을 텍스트로 디코딩\\ngenerated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\\n\\nprint(generated_text)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "text = \"ASR Recognized Text:\" + result[\"text\"]\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# 디코딩 없이 모델 추론 수행\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_length=128)\n",
    "\n",
    "# 생성된 토큰을 텍스트로 디코딩\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ko-BART\n",
    "\n",
    "https://github.com/SKT-AI/KoBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 토크나이즈 (최대 길이 제한)\\ninputs = tokenizer(\\n    text,\\n    return_tensors=\"pt\",\\n    max_length=1024,\\n    truncation=True\\n)\\ninputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\\n\\n# 텍스트 생성\\nwith torch.inference_mode():\\n    generated_ids = model.generate(inputs[\\'input_ids\\'], max_length=128)\\n\\n# 디코딩\\noutput_text = tokenizer.decode(generated_ids[0].cpu().tolist(), skip_special_tokens=True)\\nprint(output_text)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kobart-base-v1\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"skt/kobart-base-v1\").to(\"cuda\").eval()\n",
    "\"\"\"\n",
    "\n",
    "# 입력 텍스트\n",
    "#text = f\"\"\"다음 문장은 음성 인식 결과입니다. 잘못된 부분을 자연스럽게 고쳐주세요.\n",
    "#음성 인식 결과: {result[\"text\"]}\n",
    "#고쳐진 문장:\"\"\"\n",
    "\"\"\"\n",
    "# 토크나이즈 (최대 길이 제한)\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=1024,\n",
    "    truncation=True\n",
    ")\n",
    "inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "# 텍스트 생성\n",
    "with torch.inference_mode():\n",
    "    generated_ids = model.generate(inputs['input_ids'], max_length=128)\n",
    "\n",
    "# 디코딩\n",
    "output_text = tokenizer.decode(generated_ids[0].cpu().tolist(), skip_special_tokens=True)\n",
    "print(output_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ko-BERT\n",
    "\n",
    "https://github.com/SKTBrain/KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
      "The class this function is called from is 'KoBERTTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "text = f\"\"\"다음 문장은 음성 인식 결과입니다. 잘못된 부분을 자연스럽게 고쳐주세요.\n",
    "음성 인식 결과: {result[\"text\"]}\n",
    "고쳐진 문장:\"\"\"\n",
    "\n",
    "inputs = tokenizer.encode([text])\n",
    "\n",
    "out = model(input_ids = torch.tensor(inputs['input_ids']),\n",
    "              attention_mask = torch.tensor(inputs['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
